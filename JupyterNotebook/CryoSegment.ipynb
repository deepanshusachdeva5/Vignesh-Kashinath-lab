{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# !python -m venv cryo\n",
    "# !source cryo/bin/activate\n",
    "# python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dfeec0",
   "metadata": {},
   "source": [
    "### Annotations on Napari or some tool\n",
    "#### 10-20 images, annotate using the notation with background as 0, 1 - structure1, 2 - structure2, ...\n",
    "#### For each image, Annotate all the strcutures in the image\n",
    "#### Save the annotations using masks.tif (all the 10-20 images) or save mask for each images(use the mask name same as image name) as tif to a folder\n",
    "#### (Optional) Add 6-8 additional background images (These images should have only the background and no structures) to improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151698a",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread, imwrite\n",
    "import torch.optim as optim\n",
    "import mrcfile \n",
    "import qlty\n",
    "from qlty import qlty2D\n",
    "import cc3d \n",
    "import csv\n",
    "import random\n",
    "import rdp \n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import exposure,morphology\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dc3d3",
   "metadata": {},
   "source": [
    "## Specify working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db29d2",
   "metadata": {},
   "source": [
    "basedir should contain the following directories\n",
    "\n",
    "    - train_images : 10-20 images that are annotated and will be used for training\n",
    "    - train_masks : 10-20 masks that are annotated (corresponding mask should have the same name as the image)\n",
    "    - images : All the images from the tomogram\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411d59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"/data/Chromatin/MultiScale/Paper/Base/Tau_bin2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d3408",
   "metadata": {},
   "source": [
    "## Load images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dir = os.path.join(basedir, \"train_images\")\n",
    "\n",
    "\n",
    "train_imgs = []\n",
    "for f in sorted(os.listdir(train_images_dir)):\n",
    "    img = cv2.imread(os.path.join(train_images_dir, f), cv2.IMREAD_GRAYSCALE)\n",
    "    train_imgs.append(img)\n",
    "\n",
    "train_imgs = np.array(train_imgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e185d30",
   "metadata": {},
   "source": [
    "##### If mask for each image is stored in train_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbf9b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "train_masks_dir = os.path.join(basedir, \"train_masks\")\n",
    "train_masks = []\n",
    "for f in sorted(os.listdir(train_masks_dir)):\n",
    "    m = imread(os.path.join(train_masks_dir))\n",
    "    train_masks.append(train_masks)\n",
    "train_masks = np.array(train_masks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca226131",
   "metadata": {},
   "source": [
    "#### If all the masks are stored as a single .tif file, store it in base folder and read it\n",
    "(Execute only one of the previous block or the next block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d220f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = imread(os.path.join(basedir, \"masks.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb485366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_imgs = train_imgs.astype('float32')\n",
    "train_masks = train_masks.astype('uint8')\n",
    "print(train_imgs.shape, train_imgs.dtype)\n",
    "print(train_masks.shape, train_masks.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b8ac2",
   "metadata": {},
   "source": [
    "#### Verify if all the structures are annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b694f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "masks_mapper = { 1:\"Tau\", 2:\"Membrane\", 3:\"Ribosomes\"}\n",
    "unique_labels = np.unique(train_masks)\n",
    "print(\"Different Structures present in the masks:\",unique_labels)\n",
    "if len(unique_labels) < 1+len(masks_mapper.keys()):\n",
    "    raise Exception(\"Some of the structures are not annotated. Annotate them and load them again\")\n",
    "elif len(unique_labels) > 1+len(masks_mapper.keys()):\n",
    "    raise Exception(\"Add the information of all the labels(structures annotated) in masks_mapper and execute it again\")\n",
    "else:\n",
    "    for i in unique_labels:\n",
    "        if i in masks_mapper.keys():\n",
    "            print(i, masks_mapper[i])\n",
    "        elif i==0:\n",
    "            print(i, \"Background\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42532871",
   "metadata": {},
   "source": [
    "#### Randomly shuffle the order of images and masks\n",
    "(But corresponding image and masks are paired again, eventhough they might not be appeared in the order seen in the tomogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42db9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_training(imgs, masks, seed=123):\n",
    "    x = np.arange(imgs.shape[0])\n",
    "    random.seed(seed)\n",
    "    random.shuffle(x)\n",
    "    #print(x)\n",
    "    return imgs[x,:], masks[x,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25616edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.expand_dims(train_imgs, axis=1)\n",
    "train_masks = np.expand_dims(train_masks, axis=1)\n",
    "np.random.seed()\n",
    "\n",
    "train_imgs, train_masks = shuffle_training(train_imgs, train_masks, seed=None)\n",
    "print(train_imgs.shape, train_imgs.dtype)\n",
    "print(train_masks.shape, train_masks.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d07fe7",
   "metadata": {},
   "source": [
    "## Divide the images into smaller slices\n",
    "(To reduce the memory load on GPU and effectively capture all the relevant features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad978a74",
   "metadata": {},
   "source": [
    "#### We usually consider a slice of 256x256 pixels, but can be varied based on the original image size.\n",
    "#### We also use 512x512 and 1024x1024 pixels, based on the resolution of original tomogram image \n",
    "#### Most of the slices should capture some detail of the original image\n",
    "#### Eg: For original images(bin2 tomogram) of 2880x2440 pixels, we considered 1024x1024 as slice size, but for bin8 tomogram of image size 720x610 pixels, we consider 256x256 slice size\n",
    "#### With increased slice, more memory GPUs is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "quilt = qlty2D.NCYXQuilt(X=train_imgs.shape[3],\n",
    "                        Y=train_imgs.shape[2],\n",
    "                        window=(256,256),\n",
    "                        step=(64,64),\n",
    "                        border=(10,10),\n",
    "                        border_weight=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d86453",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_imgs = torch.Tensor(train_imgs)\n",
    "labeled_masks = torch.Tensor(train_masks)\n",
    "labeled_imgs, labeled_masks = quilt.unstitch_data_pair(labeled_imgs,labeled_masks)\n",
    "\n",
    "print(\"Train Images shape: \",train_imgs.shape)\n",
    "print(\"Train Masks shape: \",train_masks.shape)\n",
    "print(\"Train Image Slices shape:\", labeled_imgs.shape)\n",
    "print(\"Train Mask Slices shape:\", labeled_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4896761d",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Bilateral filter -> CLAHE\n",
    "* Bilateral filter: Noise reduction technique by smoothening/blurring the image while preserving the edges. It removes the noise by considering the similarities in spatial and pixel intensities in the neighboring pixels.\n",
    "* CLAHE (Contrast Limited Adaptive Histogram Equalization) improves the contrast and enhances the details in an image by equalizing the pixel intensity distribution of small regions while limiting the amplification of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicedImgs,dicedMasks = [],[]\n",
    "for i in range(len(labeled_imgs)):\n",
    "    # comment this to include all slices even the non annotated slices. \n",
    "    if np.unique(labeled_masks[i][0]).shape[0] > 0:\n",
    "        # bilateral filter\n",
    "        bilateral = cv2.bilateralFilter(labeled_imgs[i][0].numpy(),5,50,10)\n",
    "        # clahe equalization \n",
    "        clahe = cv2.createCLAHE(clipLimit=3)\n",
    "        bilateral= bilateral.astype(np.uint16)\n",
    "        final = clahe.apply(bilateral)\n",
    "        dicedImgs.append(final.astype(np.float32))\n",
    "        dicedMasks.append(labeled_masks[i][0].numpy())\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "train_imgs,train_masks = np.array(dicedImgs),np.array(dicedMasks)\n",
    "train_imgs,train_masks = np.expand_dims(train_imgs, axis=1),np.expand_dims(train_masks, axis=1)\n",
    "\n",
    "# %%\n",
    "print(train_imgs.shape, train_masks.shape)\n",
    "\n",
    "# %%\n",
    "labeled_imgs, labeled_masks = shuffle_training(train_imgs, train_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674afe0e",
   "metadata": {},
   "source": [
    "## Data Augmentations (Optonal)\n",
    "\n",
    "* As we have limited annotations, generate more such annotations by rotating the images. We all need to rotated the corresponding images by the same.\n",
    "* This process enhances to learn more general features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_imgs = torch.Tensor(labeled_imgs)\n",
    "labeled_masks = torch.Tensor(labeled_masks)\n",
    "rotated_imgs1 = torch.rot90(labeled_imgs, 1, [2, 3])\n",
    "rotated_masks1 = torch.rot90(labeled_masks, 1, [2, 3])\n",
    "\n",
    "rotated_imgs2 = torch.rot90(labeled_imgs, 2, [2, 3])\n",
    "rotated_masks2 = torch.rot90(labeled_masks, 2, [2, 3])\n",
    "\n",
    "rotated_imgs3 = torch.rot90(labeled_imgs, 3, [2, 3])\n",
    "rotated_masks3 = torch.rot90(labeled_masks, 3, [2, 3])\n",
    "\n",
    "flipped_imgs1 = torch.flip(labeled_imgs, [2])\n",
    "flipped_masks1 = torch.flip(labeled_masks, [2])\n",
    "\n",
    "flipped_imgs2 = torch.flip(labeled_imgs, [3])\n",
    "flipped_masks2 = torch.flip(labeled_masks, [3])\n",
    "\n",
    "flipped_imgs3 = torch.flip(labeled_imgs, [2,3])\n",
    "flipped_masks3 = torch.flip(labeled_masks, [2,3])\n",
    "\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, rotated_imgs1),0)\n",
    "labeled_masks = torch.cat((labeled_masks, rotated_masks1),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, rotated_imgs2),0)\n",
    "labeled_masks = torch.cat((labeled_masks, rotated_masks2),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, rotated_imgs3),0)\n",
    "labeled_masks = torch.cat((labeled_masks, rotated_masks3),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, flipped_imgs1),0)\n",
    "labeled_masks = torch.cat((labeled_masks, flipped_masks1),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, flipped_imgs2),0)\n",
    "labeled_masks = torch.cat((labeled_masks, flipped_masks2),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, flipped_imgs3),0)\n",
    "labeled_masks = torch.cat((labeled_masks, flipped_masks3),0)\n",
    "\n",
    "print('Shape of augmented data:    ', labeled_imgs.shape, labeled_masks.shape)\n",
    "\n",
    "labeled_imgs, labeled_masks = shuffle_training(labeled_imgs, labeled_masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce751c",
   "metadata": {},
   "source": [
    "## Train and validation data\n",
    "#### We use 2 sets of data: train and validation\n",
    "* train set: These images and masks are used for training the model\n",
    "* validation set: Using the trained model, we predict the masks on these images and compare with our manually annotated masks and have the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = int(0.05*labeled_imgs.shape[0])\n",
    "num_total = int(labeled_imgs.shape[0])\n",
    "num_train = num_total - num_val\n",
    "print('Number of images for validation: '+ str(num_val))\n",
    "val_imgs = labeled_imgs[num_train:,:,:]\n",
    "val_masks = labeled_masks[num_train:,:,:]\n",
    "train_imgs = labeled_imgs[:num_train,:,:]   # actual training\n",
    "train_masks = labeled_masks[:num_train,:,:]   # actual training\n",
    "print('Size of training data:   ', train_imgs.shape)\n",
    "print('Size of validation data: ', val_imgs.shape)\n",
    "\n",
    "num_labels = unique_labels\n",
    "print('The unique mask labels: ', num_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd84a1a",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4dccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.Tensor(train_imgs), torch.Tensor(train_masks))\n",
    "val_data = TensorDataset(torch.Tensor(val_imgs), torch.Tensor(val_masks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8affb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(train_data, val_data, \n",
    "                batch_size_train=1, batch_size_val=1):\n",
    "    \n",
    "    # can adjust the batch size depending on available memory\n",
    "    train_loader_params = {'batch_size': batch_size_train,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "    train_loader = DataLoader(train_data, **train_loader_params)\n",
    "    \n",
    "    val_loader_params = {'batch_size': batch_size_val,\n",
    "                     'shuffle': False,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "    val_loader = DataLoader(val_data, **val_loader_params)\n",
    "    \n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af76fc",
   "metadata": {},
   "source": [
    "#### batch_size : Number of slices to be processed in parallel on GPU (based on GPU memory)\n",
    "#### Commonly used valued values 16, 8, 4, 2, 1\n",
    "(Start with 16 and use next value if there is a GPU Insufficient memory error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8daf7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0   # 1 or 2 work better with CPU, 0 best for GPU\n",
    "\n",
    "# change batch size based on memory available \n",
    "batch_size_train =16\n",
    "batch_size_val = 16\n",
    "\n",
    "\n",
    "train_loader, val_loader = make_loaders(train_data,\n",
    "                                                    val_data,\n",
    "                                                    batch_size_train, \n",
    "                                                    batch_size_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76b0f9",
   "metadata": {},
   "source": [
    "#### (Optional) Analysis of how masks are distributed\n",
    "##### How many slices have a particular structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((train_masks==0).sum())\n",
    "# print((train_masks==1).sum())\n",
    "# print((train_masks==2).sum())\n",
    "# print((train_masks==3).sum())\n",
    "\n",
    "# %%\n",
    "counts=[0]*len(num_labels)\n",
    "for i in range(train_masks.shape[0]):\n",
    "    img = train_masks[i,0]\n",
    "    for j in range(len(num_labels)):\n",
    "        counts[j] += (img==j).sum()>0\n",
    "\n",
    "for j in range(len(num_labels)):\n",
    "    if j==0:\n",
    "        print(\"Slices with Background: \", counts[0],\" out of \",train_masks.shape[0])\n",
    "    else:\n",
    "        print(\"Slices with \", masks_mapper[j],\": \", counts[0],\" out of \",train_masks.shape[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49f581",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be829e",
   "metadata": {},
   "source": [
    "#### Helper Functions\n",
    "(Mostly based on DLSIA library https://dlsia.readthedocs.io/en/latest/welcome.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff33c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resulting_conv_size(Hin, dil, pad, stride, ker):\n",
    "    \"\"\"\n",
    "    Computes the resulting size of a tensor dimension given conv input parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Hin : input dimension\n",
    "    dil : dilation\n",
    "    pad : padding\n",
    "    stride : stride\n",
    "    ker : kernsel size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    the size of the resulting tensor\n",
    "\n",
    "    \"\"\"\n",
    "    N0 = (Hin + 2 * pad - dil * (ker - 1) - 1) / stride + 1\n",
    "    return int(N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resulting_convT_size(Hin, dil, pad, stride, ker, outp):\n",
    "    \"\"\"\n",
    "    Computes the resulting size of a tensor dimension given convT input parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Hin : input dimension\n",
    "    dil : dilation\n",
    "    pad : padding\n",
    "    stride : stride\n",
    "    ker : kernel size\n",
    "    outp : the outp parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    the size of the resulting tensor\n",
    "    \"\"\"\n",
    "    N0 = (Hin - 1) * stride - 2 * pad + dil * (ker - 1) + outp + 1\n",
    "    return N0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cda60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_outpadding_convT(Nsmall, Nbig, ker, stride, dil, padding):\n",
    "    \"\"\"\n",
    "    Compute the padding and output padding values neccessary for matching\n",
    "    Nsmall to Nbig dimensionality after an application of nn.ConvTranspose\n",
    "\n",
    "    :param Nsmall: small array dimensions (start)\n",
    "    :param Nbig: big array dimension (end)\n",
    "    :param ker: kernel size\n",
    "    :param stride: stride\n",
    "    :param dil: dilation\n",
    "    :param padding: padding\n",
    "    :return: the padding and output_padding\n",
    "    \"\"\"\n",
    "    tmp = stride * (Nsmall - 1) - 2 * padding + dil * (ker - 1) + 1\n",
    "    outp = Nbig - tmp\n",
    "    # outp = -(Nbig - (Nsmall - 1) * stride - 2*padding + dil * (ker - 1) - 1)\n",
    "    # outp = int(outp)\n",
    "\n",
    "    # if tmp % 2 == 0:\n",
    "    #    outp = 0\n",
    "    #    padding = int(tmp / 2)\n",
    "    # else:\n",
    "    #    outp = 1\n",
    "    #    padding = int((tmp + 1) / 2)\n",
    "    #\n",
    "    # if no_padding == True:\n",
    "    #    padding = 0\n",
    "\n",
    "    # assert padding >= 0\n",
    "    return outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd588914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outpadding_upsampling(Nsmall, Nbig, factor):\n",
    "    \"\"\"\n",
    "    Computes the extra padding value necessary for matching Nsmall to Nbig\n",
    "    dimensionality after an application of nn.Upsample\n",
    "\n",
    "    :param Nsmall: small array dimensions (start)\n",
    "    :param Nbig: big array dimension (end)\n",
    "    :param factor: the upsampling sizing factor\n",
    "    :return: the padding and output_padding\n",
    "    \"\"\"\n",
    "    tmp = Nsmall ** factor\n",
    "    outp = Nbig - tmp\n",
    "\n",
    "    return outp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_padding(dil, kernel):\n",
    "    \"\"\"\n",
    "    Do we need a function for this?\n",
    "    :param dil: Dilation\n",
    "    :param kernel: Stride\n",
    "    :return: needed padding value\n",
    "    \"\"\"\n",
    "    return int(dil * (kernel - 1) / 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_table(input_size, stride_base, min_power, max_power, kernel):\n",
    "    \"\"\"\n",
    "    A generic scaling table for a variety of possible scale change options.\n",
    "    :param input_size: input image size\n",
    "    :param stride_base: the stride_base we want to use\n",
    "    :param min_power: determines the minimum stride: stride = stride_base**min_power\n",
    "    :param max_power: determines the maximum stride: stride = stride_base**min_power\n",
    "    :param kernel: kernel size\n",
    "    :return: A dict with various settings\n",
    "    #TODO: DEBUG THIS for stride_base!=2\n",
    "    \"\"\"\n",
    "    # first establish the output sizes with respect to the input these\n",
    "    # operations are agnostic to dilation sizes as long as padding is chosen\n",
    "    # properly\n",
    "    _dil = 1\n",
    "    _pad = conv_padding(_dil, kernel)\n",
    "\n",
    "    # get sizes we need to address\n",
    "    available_sizes = []\n",
    "    powers = range(min_power, max_power + 1)\n",
    "    stride_output_padding_dict = {}\n",
    "    for power in powers:\n",
    "        # if we scale the image down, we use conv\n",
    "        if power <= 0:\n",
    "            stride = stride_base ** (-power)\n",
    "            out_size = resulting_conv_size(input_size, _dil,\n",
    "                                           _pad, stride, kernel)\n",
    "            available_sizes.append(out_size)\n",
    "            stride_output_padding_dict[power] = {}\n",
    "\n",
    "        # if we scale up we use conv_transpose\n",
    "        if power > 0:\n",
    "            stride = stride_base ** power\n",
    "            out_size = stride * input_size\n",
    "            available_sizes.append(out_size)\n",
    "            stride_output_padding_dict[power] = {}\n",
    "\n",
    "    # now we need to figure out how to go between different sizes\n",
    "\n",
    "    for ii in range(len(powers)):\n",
    "        for jj in range(len(powers)):\n",
    "            size_A = available_sizes[ii]\n",
    "            size_B = available_sizes[jj]\n",
    "            power_A = int(powers[ii])\n",
    "            power_B = int(powers[jj])\n",
    "            delta_power = power_B - power_A\n",
    "\n",
    "            # we have to scale up, so we use conv_transpose\n",
    "            if delta_power > 0:\n",
    "                stride = stride_base ** delta_power\n",
    "                add_pad = size_B - resulting_convT_size(size_A, _dil, _pad,\n",
    "                                                        stride, kernel, 0)\n",
    "                stride_output_padding_dict[power_A][power_B] = (stride,\n",
    "                                                                add_pad)\n",
    "\n",
    "            else:\n",
    "                stride = stride_base ** -delta_power\n",
    "                stride_output_padding_dict[power_A][power_B] = (stride, None)\n",
    "\n",
    "    return stride_output_padding_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8377ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_size_result(Nin, kernel, stride, dilation=1, padding=0):\n",
    "    \"\"\"\n",
    "    Determine the spatial dimension size after a max pooling operation\n",
    "\n",
    "    :param Nin: dimension of 1d array\n",
    "    :param kernel: kernel size\n",
    "    :param stride: stride; might need to match kernel size\n",
    "    :param dilation: dilation factor\n",
    "\n",
    "    :param padding: padding parameter\n",
    "    :return: the resulting array length\n",
    "    \"\"\"\n",
    "    Nout = ((Nin + 2 * padding - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "    Nout = int(Nout)\n",
    "    return Nout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdd3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unet_sizing_chart(N, depth, stride, maxpool_kernel_size,\n",
    "                      up_down_padding=0, dilation=1):\n",
    "    \"\"\"\n",
    "    Build a set of dictionaries that are useful to make sure that we can map\n",
    "    arrays back to the right sizes for each downsampling and upsampling\n",
    "    operation.\n",
    "\n",
    "    :param N: dimension of array\n",
    "    :param depth: the total depth of the unet\n",
    "    :param stride: the stride - we fix this for a single UNet\n",
    "    :param maxpool_kernel_size: the max pooling kernel size\n",
    "    :param up_down_padding: max pooling and convT padding, Default is 0\n",
    "    :param dilation: the dilation factor. default is 1\n",
    "    :return: a dictionary with information\n",
    "\n",
    "    The data associated with key \"Sizes\" provides images size per depth\n",
    "    The data associated with key \"Pool Setting\" provides info needed to\n",
    "    construct a MaxPool operator The data associated with key \"convT\n",
    "    Setting\" provides info need to construct transposed convolutions such\n",
    "    that the image of a the right size is constructed.\n",
    "\n",
    "    \"\"\"\n",
    "    resulting_sizes = {}\n",
    "    convT_settings = {}\n",
    "    pool_settings = {}\n",
    "\n",
    "    Nin = N\n",
    "    for ii in range(depth):\n",
    "        resulting_sizes[ii] = {}\n",
    "        convT_settings[ii + 1] = {}\n",
    "        pool_settings[ii] = {}\n",
    "\n",
    "        Nout = max_pool_size_result(Nin,\n",
    "                                    stride=stride,\n",
    "                                    kernel=maxpool_kernel_size,\n",
    "                                    dilation=dilation,\n",
    "                                    padding=up_down_padding\n",
    "                                    )\n",
    "        # padding=(maxpool_kernel_size - 1) / 2\n",
    "\n",
    "        pool_settings[ii][ii + 1] = {\"padding\": up_down_padding,\n",
    "                                     \"kernel\": maxpool_kernel_size,\n",
    "                                     \"dilation\": dilation,\n",
    "                                     \"stride\": stride\n",
    "                                     }\n",
    "\n",
    "        resulting_sizes[ii][ii + 1] = (Nin, Nout)\n",
    "\n",
    "        outp = get_outpadding_convT(Nout, Nin,\n",
    "                                                  dil=dilation,\n",
    "                                                  stride=stride,\n",
    "                                                  ker=maxpool_kernel_size,\n",
    "                                                  padding=up_down_padding\n",
    "                                                  )\n",
    "\n",
    "        Nup = resulting_convT_size(Nout,\n",
    "                                                 dil=dilation,\n",
    "                                                 pad=up_down_padding,\n",
    "                                                 stride=stride,\n",
    "                                                 ker=maxpool_kernel_size,\n",
    "                                                 outp=outp\n",
    "                                                 )\n",
    "\n",
    "        # assert (Nin == Nup)\n",
    "\n",
    "        convT_settings[ii + 1][ii] = {\"padding\": up_down_padding,\n",
    "                                      \"output_padding\": outp,\n",
    "                                      \"kernel\": maxpool_kernel_size,\n",
    "                                      \"dilation\": dilation,\n",
    "                                      \"stride\": stride\n",
    "                                      }\n",
    "\n",
    "        Nin = Nout\n",
    "\n",
    "    results = {\"Sizes\": resulting_sizes,\n",
    "               \"Pool_Settings\": pool_settings,\n",
    "               \"convT_settings\": convT_settings}\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_up_operator(chart, from_depth, to_depth, in_channels,\n",
    "                      out_channels, conv_kernel, key=\"convT_settings\"):\n",
    "    \"\"\"\n",
    "    Build an up sampling operator\n",
    "\n",
    "    :param chart: An array of sizing charts (one for each dimension)\n",
    "    :param from_depth: The sizing is done at this depth\n",
    "    :param to_depth: and goes to this depth\n",
    "    :param in_channels: number of input channels\n",
    "    :param out_channels: number of output channels\n",
    "    :param conv_kernel: the convolutional kernel we want to use\n",
    "    :param key: a key we can use - default is fine\n",
    "    :return: returns an operator\n",
    "    \"\"\"\n",
    "    stride = []\n",
    "    dilation = []\n",
    "    kernel = []\n",
    "    padding = []\n",
    "    output_padding = []\n",
    "\n",
    "    for ii in range(len(chart)):\n",
    "        tmp = chart[ii][key][from_depth][to_depth]\n",
    "        stride.append(tmp[\"stride\"])\n",
    "        dilation.append(tmp[\"dilation\"])\n",
    "        kernel.append(tmp[\"kernel\"])\n",
    "        padding.append(tmp[\"padding\"])\n",
    "        output_padding.append(chart[ii][key][from_depth][to_depth][\"output_padding\"])\n",
    "\n",
    "    return conv_kernel(in_channels=in_channels,\n",
    "                       out_channels=out_channels,\n",
    "                       kernel_size=kernel,\n",
    "                       stride=stride,\n",
    "                       padding=padding,\n",
    "                       output_padding=output_padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ffd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_down_operator(chart, from_depth, to_depth,\n",
    "                        maxpool_kernel, key=\"Pool_Settings\"):\n",
    "    \"\"\"\n",
    "    Build a down sampling operator\n",
    "\n",
    "    :param chart: Array of sizing charts (one for each dimension)\n",
    "    :param from_depth: we start at this depth\n",
    "    :param to_depth: and go here\n",
    "    :param maxpool_kernel: the max pooling kernel we want to use\n",
    "                                      (MaxPool2D or MaxPool3D)\n",
    "    :param key: a key we can use - default is fine\n",
    "    :return: An operator with given specs\n",
    "    \"\"\"\n",
    "    stride = []\n",
    "    dilation = []\n",
    "    kernel = []\n",
    "    padding = []\n",
    "\n",
    "    for ii in range(len(chart)):\n",
    "        tmp = chart[ii][key][from_depth][to_depth]\n",
    "        stride.append(tmp[\"stride\"])\n",
    "        dilation.append(tmp[\"dilation\"])\n",
    "        kernel.append(tmp[\"kernel\"])\n",
    "        padding.append(tmp[\"padding\"])\n",
    "\n",
    "    return maxpool_kernel(kernel_size=kernel,\n",
    "                          stride=stride,\n",
    "                          padding=padding)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497de2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352af3a",
   "metadata": {},
   "source": [
    "# TUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5fe493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This function creates a U-Net model commonly used for image semantic\n",
    "    segmentation. The model takes in an input image and outputs a segmented\n",
    "    image, with the number of output classes dictated by the out_channels\n",
    "    parameter.\n",
    "\n",
    "    In this dlsia implementation, a number of architecture-governing\n",
    "    hyperparameters may be tuned by the user, including the network depth,\n",
    "    convolutional channel growth rate both within & between layers, and the\n",
    "    normalization & activation operations following each convolution.\n",
    "\n",
    "    :param image_shape: image shape we use\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :param depth: the total depth\n",
    "    :param base_channels: the first operator take in_channels->base_channels.\n",
    "    :param growth_rate: The growth rate of number of channels per depth layer\n",
    "    :param hidden_rate: How many 'inbetween' channels do we want? This is\n",
    "                        relative to the feature channels at a given depth\n",
    "    :param conv_kernel: The convolution kernel we want to us. Conv2D or Conv3D\n",
    "    :param kernel_down: How do we steps down? MaxPool2D or MaxPool3D\n",
    "    :param kernel_up: How do we step up? nn.ConvTranspose2d or\n",
    "                      nn.ConvTranspose3d\n",
    "    :param normalization: A normalization action\n",
    "    :param activation: Activation function\n",
    "    :param conv_kernel_size: The size of the convolutional kernel we use\n",
    "    :param maxpool_kernel_size: The size of the max pooling kernel we use to\n",
    "                                step down\n",
    "    :param stride: The stride we want to use.\n",
    "    :param dilation: The dilation we want to use.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_shape,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 depth,\n",
    "                 base_channels,\n",
    "                 growth_rate=2,\n",
    "                 hidden_rate=1,\n",
    "                 conv_kernel=nn.Conv2d,\n",
    "                 kernel_down=nn.MaxPool2d,\n",
    "                 kernel_up=nn.ConvTranspose2d,\n",
    "                 normalization=nn.BatchNorm2d,\n",
    "                 activation=nn.ReLU(),\n",
    "                 final_activation = None,\n",
    "                 conv_kernel_size=3,\n",
    "                 maxpool_kernel_size=2,\n",
    "                 dilation=1\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Construct a tuneable UNet\n",
    "\n",
    "        :param image_shape: image shape we use\n",
    "        :param in_channels: input channels\n",
    "        :param out_channels: output channels\n",
    "        :param depth: the total depth\n",
    "        :param base_channels: the first operator take in_channels->base_channels.\n",
    "        :param growth_rate: The growth rate of number of channels per depth layer\n",
    "        :param hidden_rate: How many 'inbetween' channels do we want? This is\n",
    "                            relative to the feature channels at a given depth\n",
    "        :param conv_kernel: instance of PyTorch convolution class. Accepted are\n",
    "                            nn.Conv1d, nn.Conv2d, and nn.Conv3d.\n",
    "        :param kernel_down: How do we steps down? MaxPool2D or MaxPool3D\n",
    "        :param kernel_up: How do we step up? nn.ConvTranspose2d ore\n",
    "                          nn.ConvTranspose3d\n",
    "        :param normalization: PyTorch normalization class applied to each\n",
    "                              layer. Passed as class without parentheses since\n",
    "                              we need a different instance per layer.\n",
    "                              ex) normalization=nn.BatchNorm2d\n",
    "        :param activation: torch.nn class instance or list of torch.nn class\n",
    "                           instances\n",
    "        :param final_activation: torch.nn class instance or list of torch.nn class\n",
    "                           instances\n",
    "        :param conv_kernel_size: The size of the convolutional kernel we use\n",
    "        :param maxpool_kernel_size: The size of the max pooling/transposed\n",
    "                                    convolutional kernel we use in\n",
    "                                    encoder/decoder paths. Default is 2.\n",
    "        :param stride: The stride we want to use. Controls contraction/growth\n",
    "                       rates of spatial dimensions (x and y) in encoder/decoder\n",
    "                       paths. Default is 2.\n",
    "        :param dilation: The dilation we want to use.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # define the front and back of our network\n",
    "        self.image_shape = image_shape\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # determine the overall architecture\n",
    "        self.depth = depth\n",
    "        self.base_channels = base_channels\n",
    "        self.growth_rate = growth_rate\n",
    "        self.hidden_rate = hidden_rate\n",
    "\n",
    "        # These are the convolution / pooling kernels\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.kernel_down = kernel_down\n",
    "        self.kernel_up = kernel_up\n",
    "\n",
    "        # These are the convolution / pooling kernel sizes\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.maxpool_kernel_size = maxpool_kernel_size\n",
    "\n",
    "        # These control the contraction/growth rates of the spatial dimensions\n",
    "        self.stride = maxpool_kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "        # normalization and activation functions\n",
    "        if normalization is not None:\n",
    "            self.normalization = normalization\n",
    "        else:\n",
    "            self.normalization = None\n",
    "        if activation is not None:\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            self.activation = None\n",
    "\n",
    "        if final_activation is not None:\n",
    "            self.final_activation = final_activation\n",
    "        else:\n",
    "            self.final_activation = None\n",
    "\n",
    "        self.return_final_layer_ = False\n",
    "\n",
    "        # we now need to get the sizing charts sorted\n",
    "        self.sizing_chart = []\n",
    "        for N in self.image_shape:\n",
    "            self.sizing_chart.append(unet_sizing_chart(N=N,\n",
    "                                                       depth=self.depth,\n",
    "                                                       stride=self.stride,\n",
    "                                                       maxpool_kernel_size=self.maxpool_kernel_size,\n",
    "                                                       dilation=self.dilation))\n",
    "\n",
    "        # setup the layers and partial / outputs\n",
    "        self.encoder_layer_channels_in = {}\n",
    "        self.encoder_layer_channels_out = {}\n",
    "        self.encoder_layer_channels_middle = {}\n",
    "\n",
    "        self.decoder_layer_channels_in = {}\n",
    "        self.decoder_layer_channels_out = {}\n",
    "        self.decoder_layer_channels_middle = {}\n",
    "\n",
    "        self.partials_encoder = {}\n",
    "\n",
    "        self.encoders = {}\n",
    "        self.decoders = {}\n",
    "        self.step_down = {}\n",
    "        self.step_up = {}\n",
    "\n",
    "        # first pass\n",
    "        self.encoder_layer_channels_in[0] = self.in_channels\n",
    "        self.decoder_layer_channels_out[0] = self.base_channels\n",
    "\n",
    "        for ii in range(self.depth):\n",
    "\n",
    "            # Match interlayer channels for stepping down\n",
    "            if ii > 0:\n",
    "                self.encoder_layer_channels_in[ii] = self.encoder_layer_channels_out[ii - 1]\n",
    "            else:\n",
    "                self.encoder_layer_channels_middle[ii] = int(self.base_channels)\n",
    "\n",
    "            # Set base channels in first layer\n",
    "            if ii == 0:\n",
    "                self.encoder_layer_channels_middle[ii] = int(self.base_channels)\n",
    "            else:\n",
    "                self.encoder_layer_channels_middle[ii] = int(self.encoder_layer_channels_in[ii] * (self.growth_rate))\n",
    "\n",
    "            # Apply hidden rate for growth within layers\n",
    "            self.encoder_layer_channels_out[ii] = int(self.encoder_layer_channels_middle[ii] * self.hidden_rate)\n",
    "\n",
    "            # Decoder layers match Encoder channels\n",
    "\n",
    "            # Update decoder layout on 12/18/22. Vanilla version no longer\n",
    "            # contracts upon middle convolution\n",
    "            self.decoder_layer_channels_in[ii] = self.encoder_layer_channels_out[ii]\n",
    "            self.decoder_layer_channels_middle[ii] = self.encoder_layer_channels_out[ii]\n",
    "            self.decoder_layer_channels_out[ii] = self.encoder_layer_channels_middle[ii]\n",
    "\n",
    "            # self.decoder_layer_channels_in[ii] = self.encoder_layer_channels_out[ii]\n",
    "            # self.decoder_layer_channels_middle[ii] = self.encoder_layer_channels_middle[ii]\n",
    "            # self.decoder_layer_channels_out[ii] = self.encoder_layer_channels_in[ii]\n",
    "\n",
    "            self.partials_encoder[ii] = None\n",
    "\n",
    "        # Correct final decoder layer\n",
    "        self.decoder_layer_channels_out[0] = self.encoder_layer_channels_middle[0]\n",
    "\n",
    "        # Correct first decoder layer\n",
    "        self.decoder_layer_channels_in[depth - 2] = self.encoder_layer_channels_in[depth - 1]\n",
    "\n",
    "        # Second pass, add in the skip connections\n",
    "        for ii in range(depth - 1):\n",
    "            self.decoder_layer_channels_in[ii] += self.encoder_layer_channels_out[ii]\n",
    "\n",
    "        for ii in range(depth):\n",
    "\n",
    "            if ii < (depth - 1):\n",
    "\n",
    "                # Build encoder/decoder layers\n",
    "                self.encoders[ii] = \"Encode_%i\" % ii\n",
    "                tmp = self.build_unet_layer(self.encoder_layer_channels_in[ii],\n",
    "                                            self.encoder_layer_channels_middle[ii],\n",
    "                                            self.encoder_layer_channels_out[ii])\n",
    "                self.add_module(self.encoders[ii], tmp)\n",
    "\n",
    "                self.decoders[ii] = \"Decode_%i\" % ii\n",
    "\n",
    "                if ii == 0:\n",
    "                    tmp = self.build_output_layer(\n",
    "                        self.decoder_layer_channels_in[ii],\n",
    "                        self.decoder_layer_channels_middle[ii],\n",
    "                        self.decoder_layer_channels_out[ii],\n",
    "                        self.out_channels)\n",
    "                    self.add_module(self.decoders[ii], tmp)\n",
    "                else:\n",
    "                    tmp = self.build_unet_layer(self.decoder_layer_channels_in[ii],\n",
    "                                                self.decoder_layer_channels_middle[ii],\n",
    "                                                self.decoder_layer_channels_out[ii])\n",
    "                    self.add_module(self.decoders[ii], tmp)\n",
    "            else:\n",
    "                self.encoders[ii] = \"Final_layer_%i\" % ii\n",
    "                tmp = self.build_unet_layer(self.encoder_layer_channels_in[ii],\n",
    "                                            self.encoder_layer_channels_middle[\n",
    "                                                ii],\n",
    "                                            self.encoder_layer_channels_out[\n",
    "                                                ii])\n",
    "                self.add_module(self.encoders[ii], tmp)\n",
    "\n",
    "            # Build stepping operations\n",
    "            if ii < self.depth - 1:\n",
    "                # we step down like this\n",
    "                self.step_down[ii] = \"Step Down %i\" % ii\n",
    "                tmp = build_down_operator(chart=self.sizing_chart,\n",
    "                                          from_depth=ii,\n",
    "                                          to_depth=ii + 1,\n",
    "                                          maxpool_kernel=self.kernel_down,\n",
    "                                          key=\"Pool_Settings\")\n",
    "                self.add_module(self.step_down[ii], tmp)\n",
    "            if (ii >= 0) and (ii < depth - 1):\n",
    "                # we step up like this\n",
    "\n",
    "                self.step_up[ii] = \"Step Up %i\" % ii\n",
    "                if ii == (depth - 2):\n",
    "                    tmp = build_up_operator(chart=self.sizing_chart,\n",
    "                                            from_depth=ii + 1,\n",
    "                                            to_depth=ii,\n",
    "                                            in_channels=self.encoder_layer_channels_out[ii + 1],\n",
    "                                            out_channels=self.encoder_layer_channels_out[ii],\n",
    "                                            conv_kernel=self.kernel_up,\n",
    "                                            key=\"convT_settings\")\n",
    "                else:\n",
    "                    tmp = build_up_operator(chart=self.sizing_chart,\n",
    "                                            from_depth=ii + 1,\n",
    "                                            to_depth=ii,\n",
    "                                            in_channels=self.decoder_layer_channels_out[ii + 1],\n",
    "                                            out_channels=self.encoder_layer_channels_out[ii],\n",
    "                                            conv_kernel=self.kernel_up,\n",
    "                                            key=\"convT_settings\")\n",
    "\n",
    "                self.add_module(self.step_up[ii], tmp)\n",
    "\n",
    "    def build_unet_layer(self, in_channels, in_between_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Build a sequence of convolutions with activations functions and\n",
    "        normalization layers\n",
    "\n",
    "        :param in_channels: input channels\n",
    "        :param in_between_channels: the in between channels\n",
    "        :param out_channels: the output channels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Preallocate modules to house each skip connection modules\n",
    "        modules = []\n",
    "\n",
    "        # Add first convolution\n",
    "        modules.append(self.conv_kernel(in_channels,\n",
    "                                        in_between_channels,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(in_between_channels))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "\n",
    "        # Add second convolution\n",
    "        modules.append(self.conv_kernel(in_between_channels,\n",
    "                                        out_channels,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(out_channels))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "\n",
    "        # Finally, wrap all modules together in nn.Sequential\n",
    "        operator = nn.Sequential(*modules)\n",
    "\n",
    "        return operator\n",
    "\n",
    "    def build_output_layer(self, in_channels,\n",
    "                           in_between_channels1,\n",
    "                           in_between_channels2,\n",
    "                           final_channels):\n",
    "        \"\"\"\n",
    "        For final output layer, builds a sequence of convolutions with\n",
    "        activations functions and normalization layers\n",
    "\n",
    "        :param final_channels: The output channels\n",
    "        :type final_channels: int\n",
    "        :param in_channels: input channels\n",
    "        :param in_between_channels1: the in between channels after first convolution\n",
    "        :param in_between_channels2: the in between channels after second convolution\n",
    "        \"param final_channels: number of channels the network outputs\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Preallocate modules to house each skip connection modules\n",
    "        modules = []\n",
    "\n",
    "        # Add first convolution\n",
    "        modules.append(self.conv_kernel(in_channels,\n",
    "                                        in_between_channels1,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(in_between_channels1))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "\n",
    "        # Add second convolution\n",
    "        modules.append(self.conv_kernel(in_between_channels1,\n",
    "                                        in_between_channels2,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(in_between_channels2))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "\n",
    "        # Append final output convolution\n",
    "        modules.append(self.conv_kernel(in_between_channels2,\n",
    "                                        final_channels,\n",
    "                                        kernel_size=1\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Finally, wrap all modules together in nn.Sequential\n",
    "        operator = nn.Sequential(*modules)\n",
    "\n",
    "        return operator\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Default forward operator.\n",
    "\n",
    "        :param x: input tensor.\n",
    "        :return: output of neural network\n",
    "        \"\"\"\n",
    "\n",
    "        # first pass through the encoder\n",
    "        for ii in range(self.depth - 1):\n",
    "            # channel magic\n",
    "            x_out = self._modules[self.encoders[ii]](x)\n",
    "\n",
    "            # store this for decoder side processing\n",
    "            self.partials_encoder[ii] = x_out\n",
    "\n",
    "            # step down\n",
    "            x = self._modules[self.step_down[ii]](x_out)\n",
    "            # done\n",
    "\n",
    "        # last convolution in bottom, no need to stash results\n",
    "        x = self._modules[self.encoders[self.depth - 1]](x)\n",
    "\n",
    "        for ii in range(self.depth - 2, 0, -1):\n",
    "            x = self._modules[self.step_up[ii]](x)\n",
    "            x = torch.cat((self.partials_encoder[ii], x), dim=1)\n",
    "            x = self._modules[self.decoders[ii]](x)\n",
    "\n",
    "        x = self._modules[self.step_up[0]](x)\n",
    "        x = torch.cat((self.partials_encoder[0], x), dim=1)\n",
    "        x_out = self._modules[self.decoders[0]](x)\n",
    "        if self.final_activation is not None:\n",
    "            return self.final_activation(x_out)\n",
    "        return x_out\n",
    "\n",
    "    def topology_dict(self):\n",
    "        \"\"\"\n",
    "        Get all parameters needed to build this network\n",
    "\n",
    "        :return: An orderdict with all parameters needed\n",
    "        :rtype: OrderedDict\n",
    "        \"\"\"\n",
    "\n",
    "        topo_dict = OrderedDict()\n",
    "        topo_dict[\"image_shape\"] = self.image_shape\n",
    "        topo_dict[\"in_channels\"] = self.in_channels\n",
    "        topo_dict[\"out_channels\"] = self.out_channels\n",
    "        topo_dict[\"depth\"] = self.depth\n",
    "        topo_dict[\"base_channels\"] = self.base_channels\n",
    "        topo_dict[\"growth_rate\"] = self.growth_rate\n",
    "        topo_dict[\"hidden_rate\"] = self.hidden_rate\n",
    "        topo_dict[\"conv_kernel\"] = self.conv_kernel\n",
    "        topo_dict[\"kernel_down\"] = self.kernel_down\n",
    "        topo_dict[\"kernel_up\"] = self.kernel_up\n",
    "        topo_dict[\"normalization\"] = self.normalization\n",
    "        topo_dict[\"activation\"] = self.activation\n",
    "        topo_dict[\"conv_kernel_size\"] = self.conv_kernel_size\n",
    "        topo_dict[\"maxpool_kernel_size\"] = self.maxpool_kernel_size\n",
    "        topo_dict[\"dilation\"] = self.dilation\n",
    "        return topo_dict\n",
    "\n",
    "    def save_network_parameters(self, name=None):\n",
    "        \"\"\"\n",
    "        Save the network parameters\n",
    "        :param name: The filename\n",
    "        :type name: str\n",
    "        :return: None\n",
    "        :rtype: None\n",
    "        \"\"\"\n",
    "        network_dict = OrderedDict()\n",
    "        network_dict[\"topo_dict\"] = self.topology_dict()\n",
    "        network_dict[\"state_dict\"] = self.state_dict()\n",
    "        if name is None:\n",
    "            return network_dict\n",
    "        torch.save(network_dict, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351dc83",
   "metadata": {},
   "source": [
    "# Multi-UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This function creates a U-Net model commonly used for image semantic\n",
    "    segmentation. The model takes in an input image and outputs a segmented\n",
    "    image, with the number of output classes dictated by the out_channels\n",
    "    parameter.\n",
    "\n",
    "    In this implementation, a number of architecture-governing\n",
    "    hyperparameters may be tuned by the user, including the network depth,\n",
    "    convolutional channel growth rate both within & between layers, and the\n",
    "    normalization & activation operations following each convolution.\n",
    "\n",
    "    :param image_shape: image shape we use\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :param depth: the total depth\n",
    "    :param base_channels: the first operator take in_channels->base_channels.\n",
    "    :param growth_rate: The growth rate of number of channels per depth layer\n",
    "    :param hidden_rate: How many 'inbetween' channels do we want? This is\n",
    "                        relative to the feature channels at a given depth\n",
    "    :param conv_kernel: The convolution kernel we want to us. Conv2D or Conv3D\n",
    "    :param kernel_down: How do we steps down? MaxPool2D or MaxPool3D\n",
    "    :param kernel_up: How do we step up? nn.ConvTranspose2d or\n",
    "                      nn.ConvTranspose3d\n",
    "    :param normalization: A normalization action\n",
    "    :param activation: Activation function\n",
    "    :param conv_kernel_size: The size of the convolutional kernel we use\n",
    "    :param maxpool_kernel_size: The size of the max pooling kernel we use to\n",
    "                                step down\n",
    "    :param stride: The stride we want to use.\n",
    "    :param dilation: The dilation we want to use.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_shape,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 depth,\n",
    "                 base_channels,\n",
    "                 growth_rate=2,\n",
    "                 hidden_rate=1,\n",
    "                 conv_kernel=nn.Conv2d,\n",
    "                 kernel_down=nn.MaxPool2d,\n",
    "                 kernel_up=nn.ConvTranspose2d,\n",
    "                 normalization=nn.BatchNorm2d,\n",
    "                 activation=nn.ReLU(),\n",
    "                 conv_kernel_size=3,\n",
    "                 maxpool_kernel_size=2,\n",
    "                 dilation=1\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Construct a tuneable UNet\n",
    "\n",
    "        :param image_shape: image shape we use\n",
    "        :param in_channels: input channels\n",
    "        :param out_channels: output channels\n",
    "        :param depth: the total depth\n",
    "        :param base_channels: the first operator take in_channels->base_channels.\n",
    "        :param growth_rate: The growth rate of number of channels per depth layer\n",
    "        :param hidden_rate: How many 'inbetween' channels do we want? This is\n",
    "                            relative to the feature channels at a given depth\n",
    "        :param conv_kernel: instance of PyTorch convolution class. Accepted are\n",
    "                            nn.Conv1d, nn.Conv2d, and nn.Conv3d.\n",
    "        :param kernel_down: How do we steps down? MaxPool2D or MaxPool3D\n",
    "        :param kernel_up: How do we step up? nn.ConvTranspose2d ore\n",
    "                          nn.ConvTranspose3d\n",
    "        :param normalization: PyTorch normalization class applied to each\n",
    "                              layer. Passed as class without parentheses since\n",
    "                              we need a different instance per layer.\n",
    "                              ex) normalization=nn.BatchNorm2d\n",
    "        :param activation: torch.nn class instance or list of torch.nn class\n",
    "                           instances\n",
    "        :param conv_kernel_size: The size of the convolutional kernel we use\n",
    "        :param maxpool_kernel_size: The size of the max pooling/transposed\n",
    "                                    convolutional kernel we use in\n",
    "                                    encoder/decoder paths. Default is 2.\n",
    "        :param stride: The stride we want to use. Controls contraction/growth\n",
    "                       rates of spatial dimensions (x and y) in encoder/decoder\n",
    "                       paths. Default is 2.\n",
    "        :param dilation: The dilation we want to use.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # define the front and back of our network\n",
    "        self.image_shape = image_shape\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # determine the overall architecture\n",
    "        self.depth = depth\n",
    "        self.base_channels = base_channels\n",
    "        self.growth_rate = growth_rate\n",
    "        self.hidden_rate = hidden_rate\n",
    "\n",
    "        # These are the convolution / pooling kernels\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.kernel_down = kernel_down\n",
    "        self.kernel_up = kernel_up\n",
    "\n",
    "        # These are the convolution / pooling kernel sizes\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.maxpool_kernel_size = maxpool_kernel_size\n",
    "\n",
    "        # These control the contraction/growth rates of the spatial dimensions\n",
    "        self.stride = maxpool_kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "        # normalization and activation functions\n",
    "        if normalization is not None:\n",
    "            self.normalization = normalization\n",
    "        else:\n",
    "            self.normalization = None\n",
    "        if activation is not None:\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            self.activation = None\n",
    "        self.return_final_layer_ = False\n",
    "\n",
    "        # we now need to get the sizing charts sorted\n",
    "        self.sizing_chart = []\n",
    "        for N in self.image_shape:\n",
    "            self.sizing_chart.append(unet_sizing_chart(N=N,\n",
    "                                                       depth=self.depth,\n",
    "                                                       stride=self.stride,\n",
    "                                                       maxpool_kernel_size=self.maxpool_kernel_size,\n",
    "                                                       dilation=self.dilation))\n",
    "\n",
    "        # setup the layers and partial / outputs\n",
    "        self.encoder_layer_channels_in = {}\n",
    "        self.encoder_layer_channels_out = {}\n",
    "        self.encoder_layer_channels_middle = {}\n",
    "\n",
    "        self.decoder_layer_channels_in = {}\n",
    "        self.decoder_layer_channels_out = {}\n",
    "        self.decoder_layer_channels_middle = {}\n",
    "        \n",
    "        self.partials_encoder = {}\n",
    "\n",
    "        self.encoders = {}\n",
    "        self.decoders = {}\n",
    "        for i in range(out_channels):\n",
    "            self.decoders[i]={}\n",
    "        self.step_down = {}\n",
    "        self.step_up={}\n",
    "        for i in range(out_channels):\n",
    "            self.step_up[i]={}\n",
    "\n",
    "        # first pass\n",
    "        self.encoder_layer_channels_in[0] = self.in_channels\n",
    "        self.decoder_layer_channels_out[0] = self.base_channels\n",
    "\n",
    "        for ii in range(self.depth):\n",
    "\n",
    "            # Match interlayer channels for stepping down\n",
    "            if ii > 0:\n",
    "                self.encoder_layer_channels_in[ii] = self.encoder_layer_channels_out[ii - 1]\n",
    "            else:\n",
    "                self.encoder_layer_channels_middle[ii] = int(self.base_channels)\n",
    "\n",
    "            # Set base channels in first layer\n",
    "            if ii == 0:\n",
    "                self.encoder_layer_channels_middle[ii] = int(self.base_channels)\n",
    "            else:\n",
    "                self.encoder_layer_channels_middle[ii] = int(self.encoder_layer_channels_in[ii] * (self.growth_rate))\n",
    "\n",
    "            # Apply hidden rate for growth within layers\n",
    "            self.encoder_layer_channels_out[ii] = int(self.encoder_layer_channels_middle[ii] * self.hidden_rate)\n",
    "\n",
    "            # Decoder layers match Encoder channels\n",
    "\n",
    "            # Update decoder layout on 12/18/22. Vanilla version no longer\n",
    "            # contracts upon middle convolution\n",
    "            self.decoder_layer_channels_in[ii] = self.encoder_layer_channels_out[ii]\n",
    "            self.decoder_layer_channels_middle[ii] = self.encoder_layer_channels_out[ii]\n",
    "            self.decoder_layer_channels_out[ii] = self.encoder_layer_channels_middle[ii]\n",
    "\n",
    "            # self.decoder_layer_channels_in[ii] = self.encoder_layer_channels_out[ii]\n",
    "            # self.decoder_layer_channels_middle[ii] = self.encoder_layer_channels_middle[ii]\n",
    "            # self.decoder_layer_channels_out[ii] = self.encoder_layer_channels_in[ii]\n",
    "\n",
    "            self.partials_encoder[ii] = None\n",
    "\n",
    "        # Correct final decoder layer\n",
    "        self.decoder_layer_channels_out[0] = self.encoder_layer_channels_middle[0]\n",
    "\n",
    "        # Correct first decoder layer\n",
    "        self.decoder_layer_channels_in[depth - 2] = self.encoder_layer_channels_in[depth - 1]\n",
    "\n",
    "        # Second pass, add in the skip connections\n",
    "        for ii in range(depth - 1):\n",
    "            self.decoder_layer_channels_in[ii] += self.encoder_layer_channels_out[ii]\n",
    "\n",
    "        for ii in range(depth):\n",
    "\n",
    "            if ii < (depth - 1):\n",
    "\n",
    "                # Build encoder/decoder layers\n",
    "                self.encoders[ii] = \"Encode_%i\" % ii\n",
    "                tmp = self.build_unet_layer(self.encoder_layer_channels_in[ii],\n",
    "                                            self.encoder_layer_channels_middle[ii],\n",
    "                                            self.encoder_layer_channels_out[ii])\n",
    "                self.add_module(self.encoders[ii], tmp)\n",
    "                for i in range(out_channels):\n",
    "                    self.decoders[i][ii] = \"Decode_\"+str(i)+\"_\"+str(ii) \n",
    "                    if ii == 0:\n",
    "                        tmp = self.build_output_layer(\n",
    "                            self.decoder_layer_channels_in[ii],\n",
    "                            self.decoder_layer_channels_middle[ii],\n",
    "                            self.decoder_layer_channels_out[ii],\n",
    "                            #self.out_channels,\n",
    "                            1)\n",
    "                        self.add_module(self.decoders[i][ii], tmp)\n",
    "                    else:\n",
    "                        tmp = self.build_unet_layer(self.decoder_layer_channels_in[ii],\n",
    "                                                self.decoder_layer_channels_middle[ii],\n",
    "                                                self.decoder_layer_channels_out[ii])\n",
    "                        self.add_module(self.decoders[i][ii], tmp)\n",
    "            else:\n",
    "                self.encoders[ii] = \"Final_layer_%i\" % ii\n",
    "                tmp = self.build_unet_layer(self.encoder_layer_channels_in[ii],\n",
    "                                            self.encoder_layer_channels_middle[\n",
    "                                                ii],\n",
    "                                            self.encoder_layer_channels_out[\n",
    "                                                ii])\n",
    "                self.add_module(self.encoders[ii], tmp)\n",
    "\n",
    "            # Build stepping operations\n",
    "            if ii < self.depth - 1:\n",
    "                # we step down like this\n",
    "                self.step_down[ii] = \"Step Down %i\" % ii\n",
    "                tmp = build_down_operator(chart=self.sizing_chart,\n",
    "                                          from_depth=ii,\n",
    "                                          to_depth=ii + 1,\n",
    "                                          maxpool_kernel=self.kernel_down,\n",
    "                                          key=\"Pool_Settings\")\n",
    "                self.add_module(self.step_down[ii], tmp)\n",
    "            if (ii >= 0) and (ii < depth - 1):\n",
    "                # we step up like this\n",
    "                for i in range(out_channels):\n",
    "                    self.step_up[i][ii] = \"Step Up \" + str(i) + \"_\" + str(ii)\n",
    "                    if ii == (depth - 2):\n",
    "                        tmp = build_up_operator(chart=self.sizing_chart,\n",
    "                                                from_depth=ii + 1,\n",
    "                                                to_depth=ii,\n",
    "                                                in_channels=self.encoder_layer_channels_out[ii + 1],\n",
    "                                                out_channels=self.encoder_layer_channels_out[ii],\n",
    "                                                conv_kernel=self.kernel_up,\n",
    "                                                key=\"convT_settings\")\n",
    "                    else:\n",
    "                        tmp = build_up_operator(chart=self.sizing_chart,\n",
    "                                                from_depth=ii + 1,\n",
    "                                                to_depth=ii,\n",
    "                                                in_channels=self.decoder_layer_channels_out[ii + 1],\n",
    "                                                out_channels=self.encoder_layer_channels_out[ii],\n",
    "                                                conv_kernel=self.kernel_up,\n",
    "                                                key=\"convT_settings\")\n",
    "\n",
    "                    self.add_module(self.step_up[i][ii], tmp)\n",
    "\n",
    "    def build_unet_layer(self, in_channels, in_between_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Build a sequence of convolutions with activations functions and\n",
    "        normalization layers\n",
    "\n",
    "        :param in_channels: input channels\n",
    "        :param in_between_channels: the in between channels\n",
    "        :param out_channels: the output channels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Preallocate modules to house each skip connection modules\n",
    "        modules = []\n",
    "\n",
    "        # Add first convolution\n",
    "        modules.append(self.conv_kernel(in_channels,\n",
    "                                        in_between_channels,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(in_between_channels))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "\n",
    "        # Add second convolution\n",
    "        modules.append(self.conv_kernel(in_between_channels,\n",
    "                                        out_channels,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(out_channels))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "        modules.append(nn.Dropout(p=0.50))\n",
    "        # Finally, wrap all modules together in nn.Sequential\n",
    "        operator = nn.Sequential(*modules)\n",
    "\n",
    "        return operator\n",
    "\n",
    "    def build_output_layer(self, in_channels,\n",
    "                           in_between_channels1,\n",
    "                           in_between_channels2,\n",
    "                           final_channels):\n",
    "        \"\"\"\n",
    "        For final output layer, builds a sequence of convolutions with\n",
    "        activations functions and normalization layers\n",
    "\n",
    "        :param final_channels: The output channels\n",
    "        :type final_channels: int\n",
    "        :param in_channels: input channels\n",
    "        :param in_between_channels1: the in between channels after first convolution\n",
    "        :param in_between_channels2: the in between channels after second convolution\n",
    "        \"param final_channels: number of channels the network outputs\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Preallocate modules to house each skip connection modules\n",
    "        modules = []\n",
    "\n",
    "        # Add first convolution\n",
    "        modules.append(self.conv_kernel(in_channels,\n",
    "                                        in_between_channels1,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(in_between_channels1))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "\n",
    "        # Add second convolution\n",
    "        modules.append(self.conv_kernel(in_between_channels1,\n",
    "                                        in_between_channels2,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(in_between_channels2))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "        #print(\"Check\", in_between_channels1, in_between_channels2, final_channels)\n",
    "        # Append final output convolution\n",
    "        modules.append(self.conv_kernel(in_between_channels2,\n",
    "                                        final_channels,\n",
    "                                        kernel_size=1\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Finally, wrap all modules together in nn.Sequential\n",
    "        operator = nn.Sequential(*modules)\n",
    "\n",
    "        return operator\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Default forward operator.\n",
    "\n",
    "        :param x: input tensor.\n",
    "        :return: output of neural network\n",
    "        \"\"\"\n",
    "        #print(\"Start\", x.shape)\n",
    "        # first pass through the encoder\n",
    "        for ii in range(self.depth - 1):\n",
    "            # channel magic\n",
    "            x_out = self._modules[self.encoders[ii]](x)\n",
    "            #print(\" Encoder \", ii, x_out.shape)\n",
    "            # store this for decoder side processing\n",
    "            self.partials_encoder[ii] = x_out\n",
    "\n",
    "            # step down\n",
    "            if ii < self.depth-2:\n",
    "                x = self._modules[self.step_down[ii]](x_out)\n",
    "            else:\n",
    "                x_step_down = self._modules[self.step_down[ii]](x_out)\n",
    "            # done\n",
    "        xout_tensors = []\n",
    "        for i in range(self.out_channels):\n",
    "            # last convolution in bottom, no need to stash results\n",
    "            x = self._modules[self.encoders[self.depth - 1]](x_step_down)\n",
    "            #print(\"Last Encoder \", x.shape)\n",
    "            for ii in range(self.depth - 2, 0, -1):\n",
    "                x = self._modules[self.step_up[i][ii]](x)\n",
    "                x = torch.cat((self.partials_encoder[ii], x), dim=1)\n",
    "                x = self._modules[self.decoders[i][ii]](x)\n",
    "\n",
    "            x = self._modules[self.step_up[i][0]](x)\n",
    "            x = torch.cat((self.partials_encoder[0], x), dim=1)\n",
    "            #print(x.shape)\n",
    "            x_out = self._modules[self.decoders[i][0]](x)\n",
    "            xout_tensors.append(x_out)\n",
    "            \"\"\"\n",
    "            if i==0:\n",
    "                x_out = self._modules[self.decoders[i][0]](x)\n",
    "            else:\n",
    "                x_out_partial = self._modules[self.decoders[i][0]](x)\n",
    "                print(x_out.shape, x_out_partial.shape)\n",
    "                xout = torch.cat((x_out, x_out_partial),dim=1)\n",
    "            \"\"\"\n",
    "        x_out = torch.cat(xout_tensors, dim=1)\n",
    "        #print(x_out.shape)\n",
    "        return x_out\n",
    "\n",
    "    def topology_dict(self):\n",
    "        \"\"\"\n",
    "        Get all parameters needed to build this network\n",
    "\n",
    "        :return: An orderdict with all parameters needed\n",
    "        :rtype: OrderedDict\n",
    "        \"\"\"\n",
    "\n",
    "        topo_dict = OrderedDict()\n",
    "        topo_dict[\"image_shape\"] = self.image_shape\n",
    "        topo_dict[\"in_channels\"] = self.in_channels\n",
    "        topo_dict[\"out_channels\"] = self.out_channels\n",
    "        topo_dict[\"depth\"] = self.depth\n",
    "        topo_dict[\"base_channels\"] = self.base_channels\n",
    "        topo_dict[\"growth_rate\"] = self.growth_rate\n",
    "        topo_dict[\"hidden_rate\"] = self.hidden_rate\n",
    "        topo_dict[\"conv_kernel\"] = self.conv_kernel\n",
    "        topo_dict[\"kernel_down\"] = self.kernel_down\n",
    "        topo_dict[\"kernel_up\"] = self.kernel_up\n",
    "        topo_dict[\"normalization\"] = self.normalization\n",
    "        topo_dict[\"activation\"] = self.activation\n",
    "        topo_dict[\"conv_kernel_size\"] = self.conv_kernel_size\n",
    "        topo_dict[\"maxpool_kernel_size\"] = self.maxpool_kernel_size\n",
    "        topo_dict[\"dilation\"] = self.dilation\n",
    "        return topo_dict\n",
    "\n",
    "    def save_network_parameters(self, name=None):\n",
    "        \"\"\"\n",
    "        Save the network parameters\n",
    "        :param name: The filename\n",
    "        :type name: str\n",
    "        :return: None\n",
    "        :rtype: None\n",
    "        \"\"\"\n",
    "        network_dict = OrderedDict()\n",
    "        network_dict[\"topo_dict\"] = self.topology_dict()\n",
    "        network_dict[\"state_dict\"] = self.state_dict()\n",
    "        if name is None:\n",
    "            return network_dict\n",
    "        torch.save(network_dict, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71afe4e",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ac9c07",
   "metadata": {},
   "source": [
    "1. For depth, the most commonly used parameters are 4,5, and rarely 3, 6. With increased Depth, the model complexity increases and requires more memory on GPU\n",
    "2. For base_channels, the most commonly used parameters are 16, 32 and rarely 24, 48, 64. With the increased number of base channels, the model complexity increases and requires more memory on GPU\n",
    "3. For rest of parameters, growth_rate=2, hidden_rate=2, normalization=nn.BatchNorm2d , the deafult values are used.\n",
    "4. in_channels=1 for all grayscale (Black&White) images. Most of the CryoSegment used grayscale images\n",
    "5. out_channels is automatically configured to output multiples channels (grayscale images) based on the number of structures (+1 background)\n",
    "6. LEARNING_RATE is default set to 1e-3=0.001. Run experiments by increasing/decreasing the Learning rate by 0.001 and use the Learning rate with the highest performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 4\n",
    "base_channels = 32\n",
    "growth_rate = 2\n",
    "hidden_rate = 1\n",
    "in_channels = 1\n",
    "out_channels = len(num_labels)\n",
    "normalization = nn.BatchNorm2d\n",
    "LEARNING_RATE = 1e-3\n",
    "print(\"Learning Rate:\", LEARNING_RATE)\n",
    "\n",
    "\n",
    "model = MultiUNet(image_shape=(train_imgs.shape[2:4]),\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            depth=depth,\n",
    "            kernel_down=nn.MaxPool2d,\n",
    "            base_channels=base_channels,\n",
    "            normalization = nn.BatchNorm2d,\n",
    "            growth_rate=growth_rate,\n",
    "            hidden_rate=hidden_rate\n",
    "            )\n",
    "print('Number of parameters: ', sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831490c",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e810f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_model = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1aae10",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "##### (Optional) Based on the Analysis(How many slices have the structure), loss function can be weighted. Weights are choosen in the inverse order of the counts. Minimum weight possible would be 1 and the maximum value would be 5. If all the structures have same number of slices, weights are not used (i.e default would be 1 for all). Weighing the least occuring structures in slices with higher values would provide more importance for that structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = torch.FloatTensor([1,2,2,5]).to(device)\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)   # For segmenting >2 classes\n",
    "criterion = nn.CrossEntropyLoss()   # For segmenting >2 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a17b2",
   "metadata": {},
   "source": [
    "### Device (CPU/GPU)\n",
    "(If you don't have GPU, use device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f161a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda:0\"\n",
    "print('Device we will compute on: ', device)   # cuda:0 for GPU. Else, CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15895243",
   "metadata": {},
   "source": [
    "### load model onto device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)   # send network to GPU\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985db2f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab0544",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "(Specify the name of the Results directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b27b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiments = os.path.join(basedir, \"Experiments\")\n",
    "if not os.path.exists(experiments):\n",
    "    os.makedirs(experiments)\n",
    "\n",
    "newds_path = os.path.join(experiments,'Results_TauBin2_MultiUNet_Droput_TrainDown4x_8ZEROSlices_DataFlips')\n",
    "if not os.path.exists(newds_path):\n",
    "    os.makedirs(newds_path)\n",
    "model_multiunet = '/multiunet'\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "main_dir = newds_path + model_multiunet\n",
    "if os.path.isdir(main_dir) is False: os.mkdir(main_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60   # Set number of epochs\n",
    "\n",
    "stepsPerEpoch = np.ceil(train_imgs.shape[0]/batch_size_train)\n",
    "num_steps_down = 2\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_model,\n",
    "                                 step_size=int(stepsPerEpoch*(epochs/num_steps_down)),\n",
    "                                 gamma = 0.1,verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829bc70",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_metrics(preds, target, missing_label=-1, are_probs=True, num_classes=None):\n",
    "    \"\"\"\n",
    "    Computes a variety of F1 scores.\n",
    "    See : https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : Predicted labels\n",
    "    target : Target labels\n",
    "    missing_label : missing label\n",
    "    are_probs: preds are probabilities (pre or post softmax)\n",
    "    num_classes: if preds are not probabilities, we need to know the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    micro and macro F1 scores\n",
    "    \"\"\"\n",
    "\n",
    "    if are_probs:\n",
    "        num_classes = preds.shape[1]\n",
    "        tmp = torch.argmax(preds, dim=1)\n",
    "    else:\n",
    "        tmp = preds\n",
    "\n",
    "    assert num_classes is not None\n",
    "    \n",
    "    F1_eval_macro = F1Score(task='multiclass',\n",
    "                            num_classes=num_classes,\n",
    "                            average='macro')\n",
    "    F1_eval_micro = F1Score(task='multiclass',\n",
    "                           num_classes=num_classes,\n",
    "                            average='micro')    \n",
    "\n",
    "#    F1_eval_macro = F1Score(task='multiclass',\n",
    "#                            num_classes=num_classes,\n",
    "#                            average='macro')\n",
    "#    F1_eval_micro = F1Score(task='multiclass',\n",
    "#                            num_classes=num_classes,\n",
    "#                            average='micro')\n",
    "    a = tmp.cpu()\n",
    "    b = target.cpu()\n",
    "\n",
    "    sel = b == missing_label\n",
    "    a = a[~sel]\n",
    "    b = b[~sel]\n",
    "    tmp_macro = torch.Tensor([0])\n",
    "    tmp_micro = torch.Tensor([0])\n",
    "    if len(a.flatten()) > 0:\n",
    "        tmp_macro = F1_eval_macro(a, b)\n",
    "        tmp_micro = F1_eval_micro(a, b)\n",
    "\n",
    "    return tmp_micro, tmp_macro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf48f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_segmentation(net, trainloader, validationloader, NUM_EPOCHS,\n",
    "                       criterion, optimizer, device,\n",
    "                       savepath=None, saveevery=None,\n",
    "                       scheduler=None, show=0,\n",
    "                       use_amp=False, clip_value=None):\n",
    "    \"\"\"\n",
    "    Loop through epochs passing images to be segmented on a pixel-by-pixel\n",
    "    basis.\n",
    "\n",
    "    :param net: input network\n",
    "    :param trainloader: data loader with training data\n",
    "    :param validationloader: data loader with validation data\n",
    "    :param NUM_EPOCHS: number of epochs\n",
    "    :param criterion: target function\n",
    "    :param optimizer: optimization engine\n",
    "    :param device: the device where we calculate things\n",
    "    :param savepath: filepath in which we save networks intermittently\n",
    "    :param saveevery: integer n for saving network every n epochs\n",
    "    :param scheduler: an optional schedular. can be None\n",
    "    :param show: print stats every n-th epoch\n",
    "    :param use_amp: use pytorch automatic mixed precision\n",
    "    :param clip_value: value for gradient clipping. Can be None.\n",
    "    :return: A network and run summary stats\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss = []\n",
    "    F1_train_trace_micro = []\n",
    "    F1_train_trace_macro = []\n",
    "\n",
    "    # Skip validation steps if False or None loaded\n",
    "    if validationloader is False:\n",
    "        validationloader = None\n",
    "    if validationloader is not None:\n",
    "        validation_loss = []\n",
    "        F1_validation_trace_micro = []\n",
    "        F1_validation_trace_macro = []\n",
    "\n",
    "    best_score = 1e10\n",
    "    best_index = 0\n",
    "    best_state_dict = None\n",
    "\n",
    "    if savepath is not None:\n",
    "        if saveevery is None:\n",
    "            saveevery = 1\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_train_loss = 0.0\n",
    "        running_F1_train_micro = 0.0\n",
    "        running_F1_train_macro = 0.0\n",
    "        tot_train = 0.0\n",
    "\n",
    "        if validationloader is not None:\n",
    "            running_validation_loss = 0.0\n",
    "            running_F1_validation_micro = 0.0\n",
    "            running_F1_validation_macro = 0.0\n",
    "            tot_val = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for data in trainloader:\n",
    "            count += 1\n",
    "            noisy, target = data  # load noisy and target images\n",
    "            N_train = noisy.shape[0]\n",
    "            tot_train += N_train\n",
    "\n",
    "            noisy = noisy.type(torch.FloatTensor)\n",
    "            target = target.type(torch.LongTensor)\n",
    "            # print(noisy.shape, target.shape)\n",
    "            noisy = noisy.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            if criterion.__class__.__name__ == 'CrossEntropyLoss':\n",
    "                target = target.type(torch.LongTensor)\n",
    "                target = target.to(device).squeeze(1)\n",
    "\n",
    "            if use_amp is False:\n",
    "                # forward pass, compute loss and accuracy\n",
    "                output = net(noisy)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "            else:\n",
    "                scaler = torch.cuda.amp.GradScaler()\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # forward pass, compute loss and accuracy\n",
    "                    output = net(noisy)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                # backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # update the parameters\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            # update the parameters\n",
    "            if clip_value is not None:\n",
    "                torch.nn.utils.clip_grad_value_(net.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            tmp_micro, tmp_macro = segmentation_metrics(output, target)\n",
    "\n",
    "            running_F1_train_micro += tmp_micro.item()\n",
    "            running_F1_train_macro += tmp_macro.item()\n",
    "            running_train_loss += loss.item()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # compute validation step\n",
    "        if validationloader is not None:\n",
    "            with torch.no_grad():\n",
    "                for x, y in validationloader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    N_val = y.shape[0]\n",
    "                    tot_val += N_val\n",
    "                    if criterion.__class__.__name__ == 'CrossEntropyLoss':\n",
    "                        y = y.type(torch.LongTensor)\n",
    "                        y = y.to(device).squeeze(1)\n",
    "\n",
    "                    # forward pass, compute validation loss and accuracy\n",
    "                    if use_amp is False:\n",
    "                        yhat = net(x)\n",
    "                        val_loss = criterion(yhat, y)\n",
    "                    else:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            yhat = net(x)\n",
    "                            val_loss = criterion(yhat, y)\n",
    "\n",
    "                    tmp_micro, tmp_macro = segmentation_metrics(yhat, y)\n",
    "                    running_F1_validation_micro += tmp_micro.item()\n",
    "                    running_F1_validation_macro += tmp_macro.item()\n",
    "\n",
    "                    # update running validation loss and accuracy\n",
    "                    running_validation_loss += val_loss.item()\n",
    "\n",
    "        loss = running_train_loss / len(trainloader)\n",
    "        F1_micro = running_F1_train_micro / len(trainloader)\n",
    "        F1_macro = running_F1_train_macro / len(trainloader)\n",
    "        train_loss.append(loss)\n",
    "        F1_train_trace_micro.append(F1_micro)\n",
    "        F1_train_trace_macro.append(F1_macro)\n",
    "\n",
    "        if validationloader is not None:\n",
    "            val_loss = running_validation_loss / len(validationloader)\n",
    "            F1_val_micro = running_F1_validation_micro / len(validationloader)\n",
    "            F1_val_macro = running_F1_validation_macro / len(validationloader)\n",
    "            validation_loss.append(val_loss)\n",
    "            F1_validation_trace_micro.append(F1_val_micro)\n",
    "            F1_validation_trace_macro.append(F1_val_macro)\n",
    "\n",
    "        if show != 0:\n",
    "            learning_rates = []\n",
    "            for param_group in optimizer.param_groups:\n",
    "                learning_rates.append(param_group['lr'])\n",
    "            mean_learning_rate = np.mean(np.array(learning_rates))\n",
    "            if np.mod(epoch + 1, show) == 0:\n",
    "                if validationloader is not None:\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1} of {NUM_EPOCHS} | Learning rate {mean_learning_rate:4.3e}')\n",
    "                    print(\n",
    "                        f'   Training Loss: {loss:.4e} | Validation Loss: {val_loss:.4e}')\n",
    "                    print(\n",
    "                        f'   Micro Training F1: {F1_micro:.4f} | Micro Validation F1: {F1_val_micro:.4f}')\n",
    "                    print(\n",
    "                        f'   Macro Training F1: {F1_macro:.4f} | Macro Validation F1: {F1_val_macro:.4f}')\n",
    "                else:\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1} of {NUM_EPOCHS} | Learning rate {mean_learning_rate:4.3e}')\n",
    "                    print(\n",
    "                        f'   Training Loss: {loss:.4e} | Micro Training F1: {F1_micro:.4f} | Macro Training F1: {F1_macro:.4f}')\n",
    "\n",
    "        if validationloader is not None:\n",
    "            if val_loss < best_score:\n",
    "                best_state_dict = net.state_dict()\n",
    "                best_index = epoch\n",
    "                best_score = val_loss\n",
    "        else:\n",
    "            if loss < best_score:\n",
    "                best_state_dict = net.state_dict()\n",
    "                best_index = epoch\n",
    "                best_score = loss\n",
    "\n",
    "            if savepath is not None:\n",
    "                torch.save(best_state_dict, savepath + '/net_best')\n",
    "                print('   Best network found and saved')\n",
    "                print('')\n",
    "\n",
    "        if savepath is not None:\n",
    "            if np.mod(epoch + 1, saveevery) == 0:\n",
    "                torch.save(net.state_dict(), savepath + '/net_checkpoint')\n",
    "                print('   Network intermittently saved')\n",
    "                print('')\n",
    "\n",
    "    if validationloader is None:\n",
    "        validation_loss = None\n",
    "        F1_validation_trace_micro = None\n",
    "        F1_validation_trace_macro = None\n",
    "\n",
    "    results = {\"Training loss\": train_loss,\n",
    "               \"Validation loss\": validation_loss,\n",
    "               \"F1 training micro\": F1_train_trace_micro,\n",
    "               \"F1 training macro\": F1_train_trace_macro,\n",
    "               \"F1 validation micro\": F1_validation_trace_micro,\n",
    "               \"F1 validation macro\": F1_validation_trace_macro,\n",
    "               \"Best model index\": best_index}\n",
    "\n",
    "    net.load_state_dict(best_state_dict)\n",
    "    return net, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "start = time.time()\n",
    "model, results = train_segmentation(\n",
    "    model,train_loader, val_loader, epochs, \n",
    "    criterion, optimizer_model, device,saveevery=3,\n",
    "    #scheduler=scheduler,\n",
    "    show=1)   # training happens here\n",
    "\n",
    "print(\"Training Time \", time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d62c8",
   "metadata": {},
   "source": [
    "## Training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da82204",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.plot(results['F1 training macro'], linewidth=2, label='training')\n",
    "plt.plot(results['F1 validation macro'], linewidth=2, label='validation')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 macro')\n",
    "plt.title('Multi-Unet ')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(main_dir + '/losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9aade6",
   "metadata": {},
   "source": [
    "## Store trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), main_dir + '/net')\n",
    "\n",
    "# %%\n",
    "params = {'image_shape': train_imgs.shape[2:4], 'in_channels': in_channels, 'out_channels': out_channels, 'depth': depth, 'base_channels': base_channels, 'growth_rate': growth_rate, 'hidden_rate': hidden_rate},\n",
    "\n",
    "np.save(main_dir+'/params.npy',params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb1d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893847d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ec5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a468616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b40a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e497cd8",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab2384",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(model_type, params):\n",
    "    # set model parameters and initialize the network\n",
    "    if model_type == 'TUNet':\n",
    "        net = TUNet(**params)\n",
    "        return net, params\n",
    "    elif model_type == 'MultiUNet':\n",
    "        net = MultiUNet(**params)\n",
    "        return net, params\n",
    "    else:\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display(array1, array2):\n",
    "    \"\"\"\n",
    "    Displays ten random images from each one of the supplied arrays.\n",
    "    \"\"\"\n",
    "    n = 7\n",
    "\n",
    "    indices = np.random.randint(len(array1), size=n)\n",
    "    print('The indices of the images are ', indices)\n",
    "    images1 = array1[indices, :]\n",
    "    images2 = array2[indices, :]\n",
    "    plt.figure(figsize=(50, 20))\n",
    "    \n",
    "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(image1, vmin=0, vmax=1)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(image2, vmin=0, vmax=1)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_dir = main_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f47bab",
   "metadata": {},
   "source": [
    "#### Specify the model name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fcfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.load(results_dir + '/params.npy', allow_pickle=True)\n",
    "params = params[0]\n",
    "print('The following define the network parameters: ', params)\n",
    "\n",
    "\n",
    "# %%\n",
    "# model_type = 'TUNet'\n",
    "model_type = 'MultiUNet'  \n",
    "\n",
    "net, model_params = create_network(model_type, params)\n",
    "net.load_state_dict(torch.load(results_dir + '/net'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89fd16",
   "metadata": {},
   "source": [
    "#### Specify the device for prediction (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29037245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "device='cuda:0'\n",
    "print('Device we compute on: ', device)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34124494",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a693cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Specify tomogram images and predictions directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_dir = \"/data/Chromatin/MultiScale/Paper/Tau/TS30_wbp_bin2_flipped/images\"\n",
    "images_dir = os.path.join(basedir, \"images\")\n",
    "output_dir = os.path.join(experiments, \"outputs_TauBin2_MultiUNet_Droput_TrainDown4x_8ZEROSlices_DataFlips\")\n",
    "if os.path.isdir(f'{output_dir}') is False: os.mkdir(f'{output_dir}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5305e1c1",
   "metadata": {},
   "source": [
    "### Load (all) images for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a87c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
    "print('Number of files to segment: ', len(files))\n",
    "files.sort()\n",
    "\n",
    "\n",
    "test_imgs = []\n",
    "for file in files:\n",
    "    img = cv2.imread(f'{images_dir}/{file}', cv2.IMREAD_GRAYSCALE)\n",
    "    test_imgs.append(img)\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_imgs = np.expand_dims(np.array(test_imgs), axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe97fdd6",
   "metadata": {},
   "source": [
    "#### Divide the images into slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "quilt = qlty2D.NCYXQuilt(X=test_imgs.shape[3],\n",
    "                         Y=test_imgs.shape[2],\n",
    "                         window=(256,256),\n",
    "                         step=(64,64),\n",
    "                         border=(10,10),\n",
    "                         border_weight=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8232607a",
   "metadata": {},
   "source": [
    "#### Preprocessing (Bilateral + CLAHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ade3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageSplit(quilt,test_imgs):\n",
    "    dicedImgs = []\n",
    "    labeled_imgs = torch.Tensor(test_imgs)\n",
    "    labeled_imgs = quilt.unstitch(labeled_imgs)\n",
    "    \n",
    "    for i in range(len(labeled_imgs)):\n",
    "        bilateral = cv2.bilateralFilter(labeled_imgs[i][0].numpy(),5,50,10)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3)\n",
    "        bilateral= bilateral.astype(np.uint16)\n",
    "        final = clahe.apply(bilateral)\n",
    "        dicedImgs.append(final.astype(np.float32))\n",
    "    return np.expand_dims(np.array(dicedImgs), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2adeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_imgs(testloader, net):\n",
    "    \"\"\" Modified for input and no ground truth\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    seg_imgs = []\n",
    "    noisy_imgs = []\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            noisy = batch\n",
    "            noisy = noisy[0]\n",
    "            #noisy = normalize(noisy)\n",
    "            noisy = torch.FloatTensor(noisy)\n",
    "            noisy = noisy.to(device)\n",
    "            output = net(noisy)\n",
    "            output = F.softmax(output, dim=1)\n",
    "            if counter == 0:\n",
    "                seg_imgs = output.detach().cpu()\n",
    "                noisy_imgs = noisy.detach().cpu()\n",
    "            else:\n",
    "                seg_imgs = torch.cat((seg_imgs, output.detach().cpu()), 0)\n",
    "                noisy_imgs = torch.cat((noisy_imgs, noisy.detach().cpu()), 0)\n",
    "                \n",
    "            counter+=1\n",
    "            del output\n",
    "            del noisy\n",
    "            torch.cuda.empty_cache()\n",
    "    return seg_imgs, noisy_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20688241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_stack(imgx, imgy, imgz, d_type = None, return_stacks=True):\n",
    "    imgx_stack = []\n",
    "    imgy_stack = []\n",
    "    imgz_stack = []\n",
    "\n",
    "    for j in tqdm(range(len(imgx))):\n",
    "        ix = Image.open(imgx[j])\n",
    "        iy = Image.open(imgy[j])\n",
    "        iz = Image.open(imgz[j])\n",
    "        \n",
    "        ix.load()\n",
    "        iy.load()\n",
    "        iz.load()\n",
    "\n",
    "        if d_type == None:\n",
    "            ix = np.array(ix)\n",
    "            iy = np.array(iy)\n",
    "            iz = np.array(iz)\n",
    "        else:\n",
    "            ix = np.array(ix, dtype=d_type)\n",
    "            iy = np.array(iy, dtype=d_type)\n",
    "            iz = np.array(iz, dtype=d_type)\n",
    "\n",
    "        imgx_stack.append(ix)\n",
    "        imgy_stack.append(iy)\n",
    "        imgz_stack.append(iz)\n",
    "\n",
    "    imgx_stack = np.array(imgx_stack)\n",
    "    imgy_stack = np.array(imgy_stack)\n",
    "    imgz_stack = np.array(imgz_stack)\n",
    "        \n",
    "    if return_stacks == True:\n",
    "        return imgx_stack, imgy_stack, imgz_stack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# masks_mapper = {1:\"Tau\", 2:\"Membrane\", 3:\"Ribosomes\"}\n",
    "file_batch=4\n",
    "out_masks = None \n",
    "\n",
    "\n",
    "# %%\n",
    "for k,v in masks_mapper.items():\n",
    "    if os.path.isdir(f'{output_dir}/{v}') is False: os.mkdir(f'{output_dir}/{v}')\n",
    "    if os.path.isdir(f'{output_dir}/{v}/segments') is False: os.mkdir(f'{output_dir}/{v}/segments')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(0,test_imgs.shape[0],file_batch)):\n",
    "    imgs = test_imgs[i:i+file_batch]\n",
    "    #print(imgs.shape)\n",
    "    dicedtestImgs = imageSplit(quilt, imgs)\n",
    "    \n",
    "    batch_size = file_batch\n",
    "    num_workers = 0    #increase to 1 or 2 with multiple GPUs\n",
    "    test_data = TensorDataset(torch.Tensor(dicedtestImgs))\n",
    "    test_loader_params = {'batch_size': batch_size,\n",
    "                     'shuffle': False,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "    test_loader = DataLoader(test_data, **test_loader_params)  \n",
    "    \n",
    "    output, input_imgs  = segment_imgs(test_loader, net)\n",
    "    stitched_output = quilt.stitch(torch.tensor(output))\n",
    "    o = torch.squeeze(stitched_output[0], 1)\n",
    "    model_output = torch.argmax(o.cpu()[:,:,:,:].data, dim=1)\n",
    "    \n",
    "    masks=model_output.numpy()\n",
    "    imgs= np.squeeze(imgs,1)\n",
    "    \n",
    "    out_masks=masks if out_masks is None else np.vstack((out_masks,masks))\n",
    "    \n",
    "    for k,v in masks_mapper.items():\n",
    "        idx=(masks==k)\n",
    "        structures=np.zeros(imgs.shape)\n",
    "        structures[idx]=imgs[idx]\n",
    "        out_path = f'{output_dir}/{v}/segments/'\n",
    "        \n",
    "        for j in range(structures.shape[0]):\n",
    "            name = f'{i+j:04}.jpg'\n",
    "            #print(out_path+name)\n",
    "            Image.fromarray(structures[j].astype(np.uint8)).save(out_path+name)\n",
    "        \n",
    "    del output\n",
    "    del model_output\n",
    "    del input_imgs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "imwrite(output_dir+'/masks.tif', np.array(out_masks, 'uint8'))\n",
    "print(\"Testing Time \", time.time()-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ce725",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_size = 100 # remove objects smaller than this size. \n",
    "\n",
    "def clean_stack(img_stack, minim):\n",
    "        cleaned = np.copy(img_stack)\n",
    "        cleaned_index = (cleaned!=0)\n",
    "        for j in tqdm(range(len(cleaned))):\n",
    "            img = cleaned_index[j,:] \n",
    "            img = morphology.remove_small_objects(img, minim, connectivity=1)\n",
    "            target_img = cleaned[j,:,:]\n",
    "            cleaned[j,:,:] = np.multiply(target_img, img)\n",
    "        return cleaned\n",
    "\n",
    "for k,v in masks_mapper.items():\n",
    "    path = f'{output_dir}/{v}/segments'\n",
    "        \n",
    "    files = []\n",
    "    for file in glob.glob(path+\"/*.jpg\"):files.append(file)\n",
    "    files = sorted(files)\n",
    "    imgs= []\n",
    "    for j in range(len(files)):\n",
    "        img = Image.open(files[j])\n",
    "        img.load()\n",
    "        img = np.array(img, dtype='float32')\n",
    "        imgs.append(img)\n",
    "    imwrite(f'{output_dir}/{v}/{v}.tiff', clean_stack(imgs, object_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4f117",
   "metadata": {},
   "source": [
    "## Generate Co-ordinates for subtomo averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9caf520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_points(arr,z):\n",
    "    \n",
    "    final_coord = []\n",
    "    \n",
    "    count = collections.defaultdict(list)\n",
    "    rows,cols = np.nonzero(arr)\n",
    "    \n",
    "    for r,c in zip(list(rows),list(cols)):\n",
    "        pixel = arr[r][c]\n",
    "        count[pixel].append([r,c])\n",
    "        \n",
    "    for pixel, coord in count.items():\n",
    "        \n",
    "        simplied = rdp.rdp_iter(np.array(coord),epsilon=0.5)\n",
    "        simplied = [(x[0],x[1],z,pixel) for x in simplied]\n",
    "        final_coord += simplied \n",
    "    \n",
    "    return final_coord\n",
    "\n",
    "for k,v in masks_mapper.items():\n",
    "    file =f'{output_dir}/{v}/{v}.tiff'\n",
    "    \n",
    "    imgs =imread(file)\n",
    "    imgs[imgs!=0]=1\n",
    "    labels_out= cc3d.connected_components(imgs, connectivity=6)\n",
    "    total,rows,cols = labels_out.shape\n",
    "\n",
    "    with open(f'{output_dir}/{v}/coordinates.csv','a',encoding='UTF8',newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for images in range(total):\n",
    "            for val in simplify_points(labels_out[images],images):\n",
    "                writer.writerow(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02db774",
   "metadata": {},
   "source": [
    "\n",
    "## Simplify co-ordinates\n",
    "\n",
    "### Generate only one co-ordinate per feature(generates one co-ordinate for each filament or ribosome)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{output_dir}/{v}/coordinates.csv',names=[\"x\",\"y\",\"z\",\"pixel\"])\n",
    "grouped = df.groupby('pixel')\n",
    "random_points = grouped.apply(lambda x: x.iloc[np.random.randint(0,len(x))])\n",
    "random_points.to_csv(f'{output_dir}/{v}/simplified_coord.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88316411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d6e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b4ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8427013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73c3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df6f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c646792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7eb0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3bd30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c33f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15592f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401c4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6d205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
