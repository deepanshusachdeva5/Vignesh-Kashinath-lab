{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# !python -m venv cryo\n",
    "# !source cryo/bin/activate\n",
    "# python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dfeec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations on Napari or some tool\n",
    "# 10-20 images, with background as 0, 1 - structure1, 2, structure2, ...\n",
    "# masks.tif (all the 10-20 images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151698a",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread, imwrite\n",
    "import torch.optim as optim\n",
    "import mrcfile \n",
    "import qlty\n",
    "from qlty import qlty2D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from dlsia.core import helpers, train_scripts, corcoef\n",
    "from dlsia.core.networks import msdnet, tunet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dc3d3",
   "metadata": {},
   "source": [
    "## Specify working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411d59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"/data/Chromatin/MultiScale/Paper/Base/Tau_bin2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d3408",
   "metadata": {},
   "source": [
    "## Load images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = imread(os.path.join(basedir, \"train_images_down4x_0.tif\"))\n",
    "train_masks = imread(os.path.join(basedir, \"train_masks_down4x_0.tif\"))\n",
    "\n",
    "\n",
    "\n",
    "train_imgs = np.array(train_imgs)\n",
    "train_masks = np.array(train_masks)\n",
    "\n",
    "train_imgs = train_imgs.astype('float32')\n",
    "train_masks = train_masks.astype('uint8')\n",
    "print(train_imgs.shape, train_imgs.dtype)\n",
    "print(train_masks.shape, train_masks.dtype)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42db9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_training(imgs, masks, seed=123):\n",
    "    x = np.arange(imgs.shape[0])\n",
    "    random.seed(seed)\n",
    "    random.shuffle(x)\n",
    "    #print(x)\n",
    "    return imgs[x,:], masks[x,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25616edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.expand_dims(train_imgs, axis=1)\n",
    "train_masks = np.expand_dims(train_masks, axis=1)\n",
    "np.random.seed()\n",
    "\n",
    "train_imgs, train_masks = shuffle_training(train_imgs, train_masks, seed=None)\n",
    "print(train_imgs.shape, train_imgs.dtype)\n",
    "print(train_masks.shape, train_masks.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d07fe7",
   "metadata": {},
   "source": [
    "## Slit the images into smaller slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "quilt = qlty2D.NCYXQuilt(X=train_imgs.shape[3],\n",
    "                        Y=train_imgs.shape[2],\n",
    "                        window=(256,256),\n",
    "                        step=(64,64),\n",
    "                        border=(10,10),\n",
    "                        border_weight=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d86453",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_imgs = torch.Tensor(train_imgs)\n",
    "labeled_masks = torch.Tensor(train_masks)\n",
    "labeled_imgs, labeled_masks = quilt.unstitch_data_pair(labeled_imgs,labeled_masks)\n",
    "\n",
    "print(\"x shape: \",train_imgs.shape)\n",
    "print(\"y shape: \",train_masks.shape)\n",
    "print(\"x_bits shape:\", labeled_imgs.shape)\n",
    "print(\"y_bits shape:\", labeled_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4896761d",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Bilateral filter -> CLAHE -> HIST Equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicedImgs,dicedMasks = [],[]\n",
    "for i in range(len(labeled_imgs)):\n",
    "    # comment this to include all slices even the non annotated slices. \n",
    "    if np.unique(labeled_masks[i][0]).shape[0] > 0:\n",
    "        # bilateral filter\n",
    "        bilateral = cv2.bilateralFilter(labeled_imgs[i][0].numpy(),5,50,10)\n",
    "        # clahe equalization \n",
    "        clahe = cv2.createCLAHE(clipLimit=3)\n",
    "        bilateral= bilateral.astype(np.uint16)\n",
    "        final = clahe.apply(bilateral)\n",
    "        # Equalize histogram \n",
    "        x = exposure.equalize_hist(final)\n",
    "        dicedImgs.append(x.astype(np.float32))\n",
    "        dicedMasks.append(labeled_masks[i][0].numpy())\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "train_imgs,train_masks = np.array(dicedImgs),np.array(dicedMasks)\n",
    "train_imgs,train_masks = np.expand_dims(train_imgs, axis=1),np.expand_dims(train_masks, axis=1)\n",
    "\n",
    "# %%\n",
    "print(train_imgs.shape, train_masks.shape)\n",
    "\n",
    "# %%\n",
    "labeled_imgs, labeled_masks = shuffle_training(train_imgs, train_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674afe0e",
   "metadata": {},
   "source": [
    "## Data Augmentations (Optonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_imgs = torch.Tensor(labeled_imgs)\n",
    "labeled_masks = torch.Tensor(labeled_masks)\n",
    "rotated_imgs1 = torch.rot90(labeled_imgs, 1, [2, 3])\n",
    "rotated_masks1 = torch.rot90(labeled_masks, 1, [2, 3])\n",
    "\n",
    "rotated_imgs2 = torch.rot90(labeled_imgs, 2, [2, 3])\n",
    "rotated_masks2 = torch.rot90(labeled_masks, 2, [2, 3])\n",
    "\n",
    "rotated_imgs3 = torch.rot90(labeled_imgs, 3, [2, 3])\n",
    "rotated_masks3 = torch.rot90(labeled_masks, 3, [2, 3])\n",
    "\n",
    "flipped_imgs1 = torch.flip(labeled_imgs, [2])\n",
    "flipped_masks1 = torch.flip(labeled_masks, [2])\n",
    "\n",
    "flipped_imgs2 = torch.flip(labeled_imgs, [3])\n",
    "flipped_masks2 = torch.flip(labeled_masks, [3])\n",
    "\n",
    "flipped_imgs3 = torch.flip(labeled_imgs, [2,3])\n",
    "flipped_masks3 = torch.flip(labeled_masks, [2,3])\n",
    "\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, rotated_imgs1),0)\n",
    "labeled_masks = torch.cat((labeled_masks, rotated_masks1),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, rotated_imgs2),0)\n",
    "labeled_masks = torch.cat((labeled_masks, rotated_masks2),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, rotated_imgs3),0)\n",
    "labeled_masks = torch.cat((labeled_masks, rotated_masks3),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, flipped_imgs1),0)\n",
    "labeled_masks = torch.cat((labeled_masks, flipped_masks1),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, flipped_imgs2),0)\n",
    "labeled_masks = torch.cat((labeled_masks, flipped_masks2),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, flipped_imgs3),0)\n",
    "labeled_masks = torch.cat((labeled_masks, flipped_masks3),0)\n",
    "\n",
    "print('Shape of augmented data:    ', labeled_imgs.shape, labeled_masks.shape)\n",
    "\n",
    "labeled_imgs, labeled_masks = shuffle_training(labeled_imgs, labeled_masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce751c",
   "metadata": {},
   "source": [
    "# Train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = int(0.05*labeled_imgs.shape[0])\n",
    "num_total = int(labeled_imgs.shape[0])\n",
    "num_train = num_total - num_val\n",
    "print('Number of images for validation: '+ str(num_val))\n",
    "val_imgs = labeled_imgs[num_train:,:,:]\n",
    "val_masks = labeled_masks[num_train:,:,:]\n",
    "train_imgs = labeled_imgs[:num_train,:,:]   # actual training\n",
    "train_masks = labeled_masks[:num_train,:,:]   # actual training\n",
    "print('Size of training data:   ', train_imgs.shape)\n",
    "print('Size of validation data: ', val_imgs.shape)\n",
    "print('Size of testing data:    ', val_imgs.shape)\n",
    "\n",
    "num_labels = np.unique(train_masks[200:400,:])\n",
    "print('The unique mask labels: ', num_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd84a1a",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4dccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.Tensor(train_imgs), torch.Tensor(train_masks))\n",
    "val_data = TensorDataset(torch.Tensor(val_imgs), torch.Tensor(val_masks))\n",
    "test_data = TensorDataset(torch.Tensor(val_imgs), torch.Tensor(val_masks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8affb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(train_data, val_data, test_data, \n",
    "                batch_size_train, batch_size_val, batch_size_test):\n",
    "    \n",
    "    # can adjust the batch size depending on available memory\n",
    "    train_loader_params = {'batch_size': batch_size_train,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "\n",
    "    val_loader_params = {'batch_size': batch_size_val,\n",
    "                     'shuffle': False,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "\n",
    "    test_loader_params = {'batch_size': batch_size_test,\n",
    "                     'shuffle': False,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "\n",
    "    train_loader = DataLoader(train_data, **train_loader_params)\n",
    "    val_loader = DataLoader(val_data, **val_loader_params)\n",
    "    test_loader = DataLoader(test_data, **test_loader_params)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8daf7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0   # 1 or 2 work better with CPU, 0 best for GPU\n",
    "\n",
    "# change batch size based on memory available \n",
    "batch_size_train =16\n",
    "batch_size_val = 16\n",
    "batch_size_test = 16\n",
    "\n",
    "train_loader, val_loader, test_loader = make_loaders(train_data,\n",
    "                                                    val_data,\n",
    "                                                    test_data,\n",
    "                                                    batch_size_train, \n",
    "                                                    batch_size_val, \n",
    "                                                    batch_size_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76b0f9",
   "metadata": {},
   "source": [
    "## (Optional) Analysis of how masks are distributed\n",
    "### How many images have a particular structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((train_masks==0).sum())\n",
    "print((train_masks==1).sum())\n",
    "print((train_masks==2).sum())\n",
    "print((train_masks==3).sum())\n",
    "\n",
    "# %%\n",
    "counts=[0,0,0,0]\n",
    "for i in range(train_masks.shape[0]):\n",
    "    img = train_masks[i,0]\n",
    "    counts[0] += (img==0).sum()>0\n",
    "    counts[1] += (img==1).sum()>0\n",
    "    counts[2] += (img==2).sum()>0\n",
    "    counts[3] += (img==3).sum()>0\n",
    "print(counts)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49f581",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff33c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resulting_conv_size(Hin, dil, pad, stride, ker):\n",
    "    \"\"\"\n",
    "    Computes the resulting size of a tensor dimension given conv input parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Hin : input dimension\n",
    "    dil : dilation\n",
    "    pad : padding\n",
    "    stride : stride\n",
    "    ker : kernsel size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    the size of the resulting tensor\n",
    "\n",
    "    \"\"\"\n",
    "    N0 = (Hin + 2 * pad - dil * (ker - 1) - 1) / stride + 1\n",
    "    return int(N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resulting_convT_size(Hin, dil, pad, stride, ker, outp):\n",
    "    \"\"\"\n",
    "    Computes the resulting size of a tensor dimension given convT input parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Hin : input dimension\n",
    "    dil : dilation\n",
    "    pad : padding\n",
    "    stride : stride\n",
    "    ker : kernel size\n",
    "    outp : the outp parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    the size of the resulting tensor\n",
    "    \"\"\"\n",
    "    N0 = (Hin - 1) * stride - 2 * pad + dil * (ker - 1) + outp + 1\n",
    "    return N0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cda60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_outpadding_convT(Nsmall, Nbig, ker, stride, dil, padding):\n",
    "    \"\"\"\n",
    "    Compute the padding and output padding values neccessary for matching\n",
    "    Nsmall to Nbig dimensionality after an application of nn.ConvTranspose\n",
    "\n",
    "    :param Nsmall: small array dimensions (start)\n",
    "    :param Nbig: big array dimension (end)\n",
    "    :param ker: kernel size\n",
    "    :param stride: stride\n",
    "    :param dil: dilation\n",
    "    :param padding: padding\n",
    "    :return: the padding and output_padding\n",
    "    \"\"\"\n",
    "    tmp = stride * (Nsmall - 1) - 2 * padding + dil * (ker - 1) + 1\n",
    "    outp = Nbig - tmp\n",
    "    # outp = -(Nbig - (Nsmall - 1) * stride - 2*padding + dil * (ker - 1) - 1)\n",
    "    # outp = int(outp)\n",
    "\n",
    "    # if tmp % 2 == 0:\n",
    "    #    outp = 0\n",
    "    #    padding = int(tmp / 2)\n",
    "    # else:\n",
    "    #    outp = 1\n",
    "    #    padding = int((tmp + 1) / 2)\n",
    "    #\n",
    "    # if no_padding == True:\n",
    "    #    padding = 0\n",
    "\n",
    "    # assert padding >= 0\n",
    "    return outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd588914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outpadding_upsampling(Nsmall, Nbig, factor):\n",
    "    \"\"\"\n",
    "    Computes the extra padding value necessary for matching Nsmall to Nbig\n",
    "    dimensionality after an application of nn.Upsample\n",
    "\n",
    "    :param Nsmall: small array dimensions (start)\n",
    "    :param Nbig: big array dimension (end)\n",
    "    :param factor: the upsampling sizing factor\n",
    "    :return: the padding and output_padding\n",
    "    \"\"\"\n",
    "    tmp = Nsmall ** factor\n",
    "    outp = Nbig - tmp\n",
    "\n",
    "    return outp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_padding(dil, kernel):\n",
    "    \"\"\"\n",
    "    Do we need a function for this?\n",
    "    :param dil: Dilation\n",
    "    :param kernel: Stride\n",
    "    :return: needed padding value\n",
    "    \"\"\"\n",
    "    return int(dil * (kernel - 1) / 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_table(input_size, stride_base, min_power, max_power, kernel):\n",
    "    \"\"\"\n",
    "    A generic scaling table for a variety of possible scale change options.\n",
    "    :param input_size: input image size\n",
    "    :param stride_base: the stride_base we want to use\n",
    "    :param min_power: determines the minimum stride: stride = stride_base**min_power\n",
    "    :param max_power: determines the maximum stride: stride = stride_base**min_power\n",
    "    :param kernel: kernel size\n",
    "    :return: A dict with various settings\n",
    "    #TODO: DEBUG THIS for stride_base!=2\n",
    "    \"\"\"\n",
    "    # first establish the output sizes with respect to the input these\n",
    "    # operations are agnostic to dilation sizes as long as padding is chosen\n",
    "    # properly\n",
    "    _dil = 1\n",
    "    _pad = conv_padding(_dil, kernel)\n",
    "\n",
    "    # get sizes we need to address\n",
    "    available_sizes = []\n",
    "    powers = range(min_power, max_power + 1)\n",
    "    stride_output_padding_dict = {}\n",
    "    for power in powers:\n",
    "        # if we scale the image down, we use conv\n",
    "        if power <= 0:\n",
    "            stride = stride_base ** (-power)\n",
    "            out_size = resulting_conv_size(input_size, _dil,\n",
    "                                           _pad, stride, kernel)\n",
    "            available_sizes.append(out_size)\n",
    "            stride_output_padding_dict[power] = {}\n",
    "\n",
    "        # if we scale up we use conv_transpose\n",
    "        if power > 0:\n",
    "            stride = stride_base ** power\n",
    "            out_size = stride * input_size\n",
    "            available_sizes.append(out_size)\n",
    "            stride_output_padding_dict[power] = {}\n",
    "\n",
    "    # now we need to figure out how to go between different sizes\n",
    "\n",
    "    for ii in range(len(powers)):\n",
    "        for jj in range(len(powers)):\n",
    "            size_A = available_sizes[ii]\n",
    "            size_B = available_sizes[jj]\n",
    "            power_A = int(powers[ii])\n",
    "            power_B = int(powers[jj])\n",
    "            delta_power = power_B - power_A\n",
    "\n",
    "            # we have to scale up, so we use conv_transpose\n",
    "            if delta_power > 0:\n",
    "                stride = stride_base ** delta_power\n",
    "                add_pad = size_B - resulting_convT_size(size_A, _dil, _pad,\n",
    "                                                        stride, kernel, 0)\n",
    "                stride_output_padding_dict[power_A][power_B] = (stride,\n",
    "                                                                add_pad)\n",
    "\n",
    "            else:\n",
    "                stride = stride_base ** -delta_power\n",
    "                stride_output_padding_dict[power_A][power_B] = (stride, None)\n",
    "\n",
    "    return stride_output_padding_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8377ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_size_result(Nin, kernel, stride, dilation=1, padding=0):\n",
    "    \"\"\"\n",
    "    Determine the spatial dimension size after a max pooling operation\n",
    "\n",
    "    :param Nin: dimension of 1d array\n",
    "    :param kernel: kernel size\n",
    "    :param stride: stride; might need to match kernel size\n",
    "    :param dilation: dilation factor\n",
    "\n",
    "    :param padding: padding parameter\n",
    "    :return: the resulting array length\n",
    "    \"\"\"\n",
    "    Nout = ((Nin + 2 * padding - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "    Nout = int(Nout)\n",
    "    return Nout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdd3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unet_sizing_chart(N, depth, stride, maxpool_kernel_size,\n",
    "                      up_down_padding=0, dilation=1):\n",
    "    \"\"\"\n",
    "    Build a set of dictionaries that are useful to make sure that we can map\n",
    "    arrays back to the right sizes for each downsampling and upsampling\n",
    "    operation.\n",
    "\n",
    "    :param N: dimension of array\n",
    "    :param depth: the total depth of the unet\n",
    "    :param stride: the stride - we fix this for a single UNet\n",
    "    :param maxpool_kernel_size: the max pooling kernel size\n",
    "    :param up_down_padding: max pooling and convT padding, Default is 0\n",
    "    :param dilation: the dilation factor. default is 1\n",
    "    :return: a dictionary with information\n",
    "\n",
    "    The data associated with key \"Sizes\" provides images size per depth\n",
    "    The data associated with key \"Pool Setting\" provides info needed to\n",
    "    construct a MaxPool operator The data associated with key \"convT\n",
    "    Setting\" provides info need to construct transposed convolutions such\n",
    "    that the image of a the right size is constructed.\n",
    "\n",
    "    \"\"\"\n",
    "    resulting_sizes = {}\n",
    "    convT_settings = {}\n",
    "    pool_settings = {}\n",
    "\n",
    "    Nin = N\n",
    "    for ii in range(depth):\n",
    "        resulting_sizes[ii] = {}\n",
    "        convT_settings[ii + 1] = {}\n",
    "        pool_settings[ii] = {}\n",
    "\n",
    "        Nout = max_pool_size_result(Nin,\n",
    "                                    stride=stride,\n",
    "                                    kernel=maxpool_kernel_size,\n",
    "                                    dilation=dilation,\n",
    "                                    padding=up_down_padding\n",
    "                                    )\n",
    "        # padding=(maxpool_kernel_size - 1) / 2\n",
    "\n",
    "        pool_settings[ii][ii + 1] = {\"padding\": up_down_padding,\n",
    "                                     \"kernel\": maxpool_kernel_size,\n",
    "                                     \"dilation\": dilation,\n",
    "                                     \"stride\": stride\n",
    "                                     }\n",
    "\n",
    "        resulting_sizes[ii][ii + 1] = (Nin, Nout)\n",
    "\n",
    "        outp = get_outpadding_convT(Nout, Nin,\n",
    "                                                  dil=dilation,\n",
    "                                                  stride=stride,\n",
    "                                                  ker=maxpool_kernel_size,\n",
    "                                                  padding=up_down_padding\n",
    "                                                  )\n",
    "\n",
    "        Nup = resulting_convT_size(Nout,\n",
    "                                                 dil=dilation,\n",
    "                                                 pad=up_down_padding,\n",
    "                                                 stride=stride,\n",
    "                                                 ker=maxpool_kernel_size,\n",
    "                                                 outp=outp\n",
    "                                                 )\n",
    "\n",
    "        # assert (Nin == Nup)\n",
    "\n",
    "        convT_settings[ii + 1][ii] = {\"padding\": up_down_padding,\n",
    "                                      \"output_padding\": outp,\n",
    "                                      \"kernel\": maxpool_kernel_size,\n",
    "                                      \"dilation\": dilation,\n",
    "                                      \"stride\": stride\n",
    "                                      }\n",
    "\n",
    "        Nin = Nout\n",
    "\n",
    "    results = {\"Sizes\": resulting_sizes,\n",
    "               \"Pool_Settings\": pool_settings,\n",
    "               \"convT_settings\": convT_settings}\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_up_operator(chart, from_depth, to_depth, in_channels,\n",
    "                      out_channels, conv_kernel, key=\"convT_settings\"):\n",
    "    \"\"\"\n",
    "    Build an up sampling operator\n",
    "\n",
    "    :param chart: An array of sizing charts (one for each dimension)\n",
    "    :param from_depth: The sizing is done at this depth\n",
    "    :param to_depth: and goes to this depth\n",
    "    :param in_channels: number of input channels\n",
    "    :param out_channels: number of output channels\n",
    "    :param conv_kernel: the convolutional kernel we want to use\n",
    "    :param key: a key we can use - default is fine\n",
    "    :return: returns an operator\n",
    "    \"\"\"\n",
    "    stride = []\n",
    "    dilation = []\n",
    "    kernel = []\n",
    "    padding = []\n",
    "    output_padding = []\n",
    "\n",
    "    for ii in range(len(chart)):\n",
    "        tmp = chart[ii][key][from_depth][to_depth]\n",
    "        stride.append(tmp[\"stride\"])\n",
    "        dilation.append(tmp[\"dilation\"])\n",
    "        kernel.append(tmp[\"kernel\"])\n",
    "        padding.append(tmp[\"padding\"])\n",
    "        output_padding.append(chart[ii][key][from_depth][to_depth][\"output_padding\"])\n",
    "\n",
    "    return conv_kernel(in_channels=in_channels,\n",
    "                       out_channels=out_channels,\n",
    "                       kernel_size=kernel,\n",
    "                       stride=stride,\n",
    "                       padding=padding,\n",
    "                       output_padding=output_padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ffd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_down_operator(chart, from_depth, to_depth,\n",
    "                        maxpool_kernel, key=\"Pool_Settings\"):\n",
    "    \"\"\"\n",
    "    Build a down sampling operator\n",
    "\n",
    "    :param chart: Array of sizing charts (one for each dimension)\n",
    "    :param from_depth: we start at this depth\n",
    "    :param to_depth: and go here\n",
    "    :param maxpool_kernel: the max pooling kernel we want to use\n",
    "                                      (MaxPool2D or MaxPool3D)\n",
    "    :param key: a key we can use - default is fine\n",
    "    :return: An operator with given specs\n",
    "    \"\"\"\n",
    "    stride = []\n",
    "    dilation = []\n",
    "    kernel = []\n",
    "    padding = []\n",
    "\n",
    "    for ii in range(len(chart)):\n",
    "        tmp = chart[ii][key][from_depth][to_depth]\n",
    "        stride.append(tmp[\"stride\"])\n",
    "        dilation.append(tmp[\"dilation\"])\n",
    "        kernel.append(tmp[\"kernel\"])\n",
    "        padding.append(tmp[\"padding\"])\n",
    "\n",
    "    return maxpool_kernel(kernel_size=kernel,\n",
    "                          stride=stride,\n",
    "                          padding=padding)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497de2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352af3a",
   "metadata": {},
   "source": [
    "# Multi-UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiTUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This function creates a U-Net model commonly used for image semantic\n",
    "    segmentation. The model takes in an input image and outputs a segmented\n",
    "    image, with the number of output classes dictated by the out_channels\n",
    "    parameter.\n",
    "\n",
    "    In this dlsia implementation, a number of architecture-governing\n",
    "    hyperparameters may be tuned by the user, including the network depth,\n",
    "    convolutional channel growth rate both within & between layers, and the\n",
    "    normalization & activation operations following each convolution.\n",
    "\n",
    "    :param image_shape: image shape we use\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :param depth: the total depth\n",
    "    :param base_channels: the first operator take in_channels->base_channels.\n",
    "    :param growth_rate: The growth rate of number of channels per depth layer\n",
    "    :param hidden_rate: How many 'inbetween' channels do we want? This is\n",
    "                        relative to the feature channels at a given depth\n",
    "    :param conv_kernel: The convolution kernel we want to us. Conv2D or Conv3D\n",
    "    :param kernel_down: How do we steps down? MaxPool2D or MaxPool3D\n",
    "    :param kernel_up: How do we step up? nn.ConvTranspose2d or\n",
    "                      nn.ConvTranspose3d\n",
    "    :param normalization: A normalization action\n",
    "    :param activation: Activation function\n",
    "    :param conv_kernel_size: The size of the convolutional kernel we use\n",
    "    :param maxpool_kernel_size: The size of the max pooling kernel we use to\n",
    "                                step down\n",
    "    :param stride: The stride we want to use.\n",
    "    :param dilation: The dilation we want to use.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_shape,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 depth,\n",
    "                 base_channels,\n",
    "                 growth_rate=2,\n",
    "                 hidden_rate=1,\n",
    "                 conv_kernel=nn.Conv2d,\n",
    "                 kernel_down=nn.MaxPool2d,\n",
    "                 kernel_up=nn.ConvTranspose2d,\n",
    "                 normalization=nn.BatchNorm2d,\n",
    "                 activation=nn.ReLU(),\n",
    "                 conv_kernel_size=3,\n",
    "                 maxpool_kernel_size=2,\n",
    "                 dilation=1\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Construct a tuneable UNet\n",
    "\n",
    "        :param image_shape: image shape we use\n",
    "        :param in_channels: input channels\n",
    "        :param out_channels: output channels\n",
    "        :param depth: the total depth\n",
    "        :param base_channels: the first operator take in_channels->base_channels.\n",
    "        :param growth_rate: The growth rate of number of channels per depth layer\n",
    "        :param hidden_rate: How many 'inbetween' channels do we want? This is\n",
    "                            relative to the feature channels at a given depth\n",
    "        :param conv_kernel: instance of PyTorch convolution class. Accepted are\n",
    "                            nn.Conv1d, nn.Conv2d, and nn.Conv3d.\n",
    "        :param kernel_down: How do we steps down? MaxPool2D or MaxPool3D\n",
    "        :param kernel_up: How do we step up? nn.ConvTranspose2d ore\n",
    "                          nn.ConvTranspose3d\n",
    "        :param normalization: PyTorch normalization class applied to each\n",
    "                              layer. Passed as class without parentheses since\n",
    "                              we need a different instance per layer.\n",
    "                              ex) normalization=nn.BatchNorm2d\n",
    "        :param activation: torch.nn class instance or list of torch.nn class\n",
    "                           instances\n",
    "        :param conv_kernel_size: The size of the convolutional kernel we use\n",
    "        :param maxpool_kernel_size: The size of the max pooling/transposed\n",
    "                                    convolutional kernel we use in\n",
    "                                    encoder/decoder paths. Default is 2.\n",
    "        :param stride: The stride we want to use. Controls contraction/growth\n",
    "                       rates of spatial dimensions (x and y) in encoder/decoder\n",
    "                       paths. Default is 2.\n",
    "        :param dilation: The dilation we want to use.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # define the front and back of our network\n",
    "        self.image_shape = image_shape\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # determine the overall architecture\n",
    "        self.depth = depth\n",
    "        self.base_channels = base_channels\n",
    "        self.growth_rate = growth_rate\n",
    "        self.hidden_rate = hidden_rate\n",
    "\n",
    "        # These are the convolution / pooling kernels\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.kernel_down = kernel_down\n",
    "        self.kernel_up = kernel_up\n",
    "\n",
    "        # These are the convolution / pooling kernel sizes\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.maxpool_kernel_size = maxpool_kernel_size\n",
    "\n",
    "        # These control the contraction/growth rates of the spatial dimensions\n",
    "        self.stride = maxpool_kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "        # normalization and activation functions\n",
    "        if normalization is not None:\n",
    "            self.normalization = normalization\n",
    "        else:\n",
    "            self.normalization = None\n",
    "        if activation is not None:\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            self.activation = None\n",
    "        self.return_final_layer_ = False\n",
    "\n",
    "        # we now need to get the sizing charts sorted\n",
    "        self.sizing_chart = []\n",
    "        for N in self.image_shape:\n",
    "            self.sizing_chart.append(unet_sizing_chart(N=N,\n",
    "                                                       depth=self.depth,\n",
    "                                                       stride=self.stride,\n",
    "                                                       maxpool_kernel_size=self.maxpool_kernel_size,\n",
    "                                                       dilation=self.dilation))\n",
    "\n",
    "        # setup the layers and partial / outputs\n",
    "        self.encoder_layer_channels_in = {}\n",
    "        self.encoder_layer_channels_out = {}\n",
    "        self.encoder_layer_channels_middle = {}\n",
    "\n",
    "        self.decoder_layer_channels_in = {}\n",
    "        self.decoder_layer_channels_out = {}\n",
    "        self.decoder_layer_channels_middle = {}\n",
    "        \n",
    "        self.partials_encoder = {}\n",
    "\n",
    "        self.encoders = {}\n",
    "        self.decoders = {}\n",
    "        for i in range(out_channels):\n",
    "            self.decoders[i]={}\n",
    "        self.step_down = {}\n",
    "        self.step_up={}\n",
    "        for i in range(out_channels):\n",
    "            self.step_up[i]={}\n",
    "\n",
    "        # first pass\n",
    "        self.encoder_layer_channels_in[0] = self.in_channels\n",
    "        self.decoder_layer_channels_out[0] = self.base_channels\n",
    "\n",
    "        for ii in range(self.depth):\n",
    "\n",
    "            # Match interlayer channels for stepping down\n",
    "            if ii > 0:\n",
    "                self.encoder_layer_channels_in[ii] = self.encoder_layer_channels_out[ii - 1]\n",
    "            else:\n",
    "                self.encoder_layer_channels_middle[ii] = int(self.base_channels)\n",
    "\n",
    "            # Set base channels in first layer\n",
    "            if ii == 0:\n",
    "                self.encoder_layer_channels_middle[ii] = int(self.base_channels)\n",
    "            else:\n",
    "                self.encoder_layer_channels_middle[ii] = int(self.encoder_layer_channels_in[ii] * (self.growth_rate))\n",
    "\n",
    "            # Apply hidden rate for growth within layers\n",
    "            self.encoder_layer_channels_out[ii] = int(self.encoder_layer_channels_middle[ii] * self.hidden_rate)\n",
    "\n",
    "            # Decoder layers match Encoder channels\n",
    "\n",
    "            # Update decoder layout on 12/18/22. Vanilla version no longer\n",
    "            # contracts upon middle convolution\n",
    "            self.decoder_layer_channels_in[ii] = self.encoder_layer_channels_out[ii]\n",
    "            self.decoder_layer_channels_middle[ii] = self.encoder_layer_channels_out[ii]\n",
    "            self.decoder_layer_channels_out[ii] = self.encoder_layer_channels_middle[ii]\n",
    "\n",
    "            # self.decoder_layer_channels_in[ii] = self.encoder_layer_channels_out[ii]\n",
    "            # self.decoder_layer_channels_middle[ii] = self.encoder_layer_channels_middle[ii]\n",
    "            # self.decoder_layer_channels_out[ii] = self.encoder_layer_channels_in[ii]\n",
    "\n",
    "            self.partials_encoder[ii] = None\n",
    "\n",
    "        # Correct final decoder layer\n",
    "        self.decoder_layer_channels_out[0] = self.encoder_layer_channels_middle[0]\n",
    "\n",
    "        # Correct first decoder layer\n",
    "        self.decoder_layer_channels_in[depth - 2] = self.encoder_layer_channels_in[depth - 1]\n",
    "\n",
    "        # Second pass, add in the skip connections\n",
    "        for ii in range(depth - 1):\n",
    "            self.decoder_layer_channels_in[ii] += self.encoder_layer_channels_out[ii]\n",
    "\n",
    "        for ii in range(depth):\n",
    "\n",
    "            if ii < (depth - 1):\n",
    "\n",
    "                # Build encoder/decoder layers\n",
    "                self.encoders[ii] = \"Encode_%i\" % ii\n",
    "                tmp = self.build_unet_layer(self.encoder_layer_channels_in[ii],\n",
    "                                            self.encoder_layer_channels_middle[ii],\n",
    "                                            self.encoder_layer_channels_out[ii])\n",
    "                self.add_module(self.encoders[ii], tmp)\n",
    "                for i in range(out_channels):\n",
    "                    self.decoders[i][ii] = \"Decode_\"+str(i)+\"_\"+str(ii) \n",
    "                    if ii == 0:\n",
    "                        tmp = self.build_output_layer(\n",
    "                            self.decoder_layer_channels_in[ii],\n",
    "                            self.decoder_layer_channels_middle[ii],\n",
    "                            self.decoder_layer_channels_out[ii],\n",
    "                            #self.out_channels,\n",
    "                            1)\n",
    "                        self.add_module(self.decoders[i][ii], tmp)\n",
    "                    else:\n",
    "                        tmp = self.build_unet_layer(self.decoder_layer_channels_in[ii],\n",
    "                                                self.decoder_layer_channels_middle[ii],\n",
    "                                                self.decoder_layer_channels_out[ii])\n",
    "                        self.add_module(self.decoders[i][ii], tmp)\n",
    "            else:\n",
    "                self.encoders[ii] = \"Final_layer_%i\" % ii\n",
    "                tmp = self.build_unet_layer(self.encoder_layer_channels_in[ii],\n",
    "                                            self.encoder_layer_channels_middle[\n",
    "                                                ii],\n",
    "                                            self.encoder_layer_channels_out[\n",
    "                                                ii])\n",
    "                self.add_module(self.encoders[ii], tmp)\n",
    "\n",
    "            # Build stepping operations\n",
    "            if ii < self.depth - 1:\n",
    "                # we step down like this\n",
    "                self.step_down[ii] = \"Step Down %i\" % ii\n",
    "                tmp = build_down_operator(chart=self.sizing_chart,\n",
    "                                          from_depth=ii,\n",
    "                                          to_depth=ii + 1,\n",
    "                                          maxpool_kernel=self.kernel_down,\n",
    "                                          key=\"Pool_Settings\")\n",
    "                self.add_module(self.step_down[ii], tmp)\n",
    "            if (ii >= 0) and (ii < depth - 1):\n",
    "                # we step up like this\n",
    "                for i in range(out_channels):\n",
    "                    self.step_up[i][ii] = \"Step Up \" + str(i) + \"_\" + str(ii)\n",
    "                    if ii == (depth - 2):\n",
    "                        tmp = build_up_operator(chart=self.sizing_chart,\n",
    "                                                from_depth=ii + 1,\n",
    "                                                to_depth=ii,\n",
    "                                                in_channels=self.encoder_layer_channels_out[ii + 1],\n",
    "                                                out_channels=self.encoder_layer_channels_out[ii],\n",
    "                                                conv_kernel=self.kernel_up,\n",
    "                                                key=\"convT_settings\")\n",
    "                    else:\n",
    "                        tmp = build_up_operator(chart=self.sizing_chart,\n",
    "                                                from_depth=ii + 1,\n",
    "                                                to_depth=ii,\n",
    "                                                in_channels=self.decoder_layer_channels_out[ii + 1],\n",
    "                                                out_channels=self.encoder_layer_channels_out[ii],\n",
    "                                                conv_kernel=self.kernel_up,\n",
    "                                                key=\"convT_settings\")\n",
    "\n",
    "                    self.add_module(self.step_up[i][ii], tmp)\n",
    "\n",
    "    def build_unet_layer(self, in_channels, in_between_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Build a sequence of convolutions with activations functions and\n",
    "        normalization layers\n",
    "\n",
    "        :param in_channels: input channels\n",
    "        :param in_between_channels: the in between channels\n",
    "        :param out_channels: the output channels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Preallocate modules to house each skip connection modules\n",
    "        modules = []\n",
    "\n",
    "        # Add first convolution\n",
    "        modules.append(self.conv_kernel(in_channels,\n",
    "                                        in_between_channels,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(in_between_channels))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "\n",
    "        # Add second convolution\n",
    "        modules.append(self.conv_kernel(in_between_channels,\n",
    "                                        out_channels,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(out_channels))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "        modules.append(nn.Dropout(p=0.50))\n",
    "        # Finally, wrap all modules together in nn.Sequential\n",
    "        operator = nn.Sequential(*modules)\n",
    "\n",
    "        return operator\n",
    "\n",
    "    def build_output_layer(self, in_channels,\n",
    "                           in_between_channels1,\n",
    "                           in_between_channels2,\n",
    "                           final_channels):\n",
    "        \"\"\"\n",
    "        For final output layer, builds a sequence of convolutions with\n",
    "        activations functions and normalization layers\n",
    "\n",
    "        :param final_channels: The output channels\n",
    "        :type final_channels: int\n",
    "        :param in_channels: input channels\n",
    "        :param in_between_channels1: the in between channels after first convolution\n",
    "        :param in_between_channels2: the in between channels after second convolution\n",
    "        \"param final_channels: number of channels the network outputs\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Preallocate modules to house each skip connection modules\n",
    "        modules = []\n",
    "\n",
    "        # Add first convolution\n",
    "        modules.append(self.conv_kernel(in_channels,\n",
    "                                        in_between_channels1,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(in_between_channels1))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "\n",
    "        # Add second convolution\n",
    "        modules.append(self.conv_kernel(in_between_channels1,\n",
    "                                        in_between_channels2,\n",
    "                                        kernel_size=self.conv_kernel_size,\n",
    "                                        padding=int((self.conv_kernel_size - 1) / 2)\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Append normalization/activation bundle, if applicable\n",
    "        if self.normalization is not None:\n",
    "            modules.append(self.normalization(in_between_channels2))\n",
    "        if self.activation is not None:\n",
    "            modules.append(self.activation)\n",
    "        #print(\"Check\", in_between_channels1, in_between_channels2, final_channels)\n",
    "        # Append final output convolution\n",
    "        modules.append(self.conv_kernel(in_between_channels2,\n",
    "                                        final_channels,\n",
    "                                        kernel_size=1\n",
    "                                        )\n",
    "                       )\n",
    "\n",
    "        # Finally, wrap all modules together in nn.Sequential\n",
    "        operator = nn.Sequential(*modules)\n",
    "\n",
    "        return operator\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Default forward operator.\n",
    "\n",
    "        :param x: input tensor.\n",
    "        :return: output of neural network\n",
    "        \"\"\"\n",
    "        #print(\"Start\", x.shape)\n",
    "        # first pass through the encoder\n",
    "        for ii in range(self.depth - 1):\n",
    "            # channel magic\n",
    "            x_out = self._modules[self.encoders[ii]](x)\n",
    "            #print(\" Encoder \", ii, x_out.shape)\n",
    "            # store this for decoder side processing\n",
    "            self.partials_encoder[ii] = x_out\n",
    "\n",
    "            # step down\n",
    "            if ii < self.depth-2:\n",
    "                x = self._modules[self.step_down[ii]](x_out)\n",
    "            else:\n",
    "                x_step_down = self._modules[self.step_down[ii]](x_out)\n",
    "            # done\n",
    "        xout_tensors = []\n",
    "        for i in range(self.out_channels):\n",
    "            # last convolution in bottom, no need to stash results\n",
    "            x = self._modules[self.encoders[self.depth - 1]](x_step_down)\n",
    "            #print(\"Last Encoder \", x.shape)\n",
    "            for ii in range(self.depth - 2, 0, -1):\n",
    "                x = self._modules[self.step_up[i][ii]](x)\n",
    "                x = torch.cat((self.partials_encoder[ii], x), dim=1)\n",
    "                x = self._modules[self.decoders[i][ii]](x)\n",
    "\n",
    "            x = self._modules[self.step_up[i][0]](x)\n",
    "            x = torch.cat((self.partials_encoder[0], x), dim=1)\n",
    "            #print(x.shape)\n",
    "            x_out = self._modules[self.decoders[i][0]](x)\n",
    "            xout_tensors.append(x_out)\n",
    "            \"\"\"\n",
    "            if i==0:\n",
    "                x_out = self._modules[self.decoders[i][0]](x)\n",
    "            else:\n",
    "                x_out_partial = self._modules[self.decoders[i][0]](x)\n",
    "                print(x_out.shape, x_out_partial.shape)\n",
    "                xout = torch.cat((x_out, x_out_partial),dim=1)\n",
    "            \"\"\"\n",
    "        x_out = torch.cat(xout_tensors, dim=1)\n",
    "        #print(x_out.shape)\n",
    "        return x_out\n",
    "\n",
    "    def topology_dict(self):\n",
    "        \"\"\"\n",
    "        Get all parameters needed to build this network\n",
    "\n",
    "        :return: An orderdict with all parameters needed\n",
    "        :rtype: OrderedDict\n",
    "        \"\"\"\n",
    "\n",
    "        topo_dict = OrderedDict()\n",
    "        topo_dict[\"image_shape\"] = self.image_shape\n",
    "        topo_dict[\"in_channels\"] = self.in_channels\n",
    "        topo_dict[\"out_channels\"] = self.out_channels\n",
    "        topo_dict[\"depth\"] = self.depth\n",
    "        topo_dict[\"base_channels\"] = self.base_channels\n",
    "        topo_dict[\"growth_rate\"] = self.growth_rate\n",
    "        topo_dict[\"hidden_rate\"] = self.hidden_rate\n",
    "        topo_dict[\"conv_kernel\"] = self.conv_kernel\n",
    "        topo_dict[\"kernel_down\"] = self.kernel_down\n",
    "        topo_dict[\"kernel_up\"] = self.kernel_up\n",
    "        topo_dict[\"normalization\"] = self.normalization\n",
    "        topo_dict[\"activation\"] = self.activation\n",
    "        topo_dict[\"conv_kernel_size\"] = self.conv_kernel_size\n",
    "        topo_dict[\"maxpool_kernel_size\"] = self.maxpool_kernel_size\n",
    "        topo_dict[\"dilation\"] = self.dilation\n",
    "        return topo_dict\n",
    "\n",
    "    def save_network_parameters(self, name=None):\n",
    "        \"\"\"\n",
    "        Save the network parameters\n",
    "        :param name: The filename\n",
    "        :type name: str\n",
    "        :return: None\n",
    "        :rtype: None\n",
    "        \"\"\"\n",
    "        network_dict = OrderedDict()\n",
    "        network_dict[\"topo_dict\"] = self.topology_dict()\n",
    "        network_dict[\"state_dict\"] = self.state_dict()\n",
    "        if name is None:\n",
    "            return network_dict\n",
    "        torch.save(network_dict, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71afe4e",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 4\n",
    "base_channels = 32\n",
    "growth_rate = 2\n",
    "hidden_rate = 1\n",
    "in_channels = 1\n",
    "out_channels = len(num_labels)\n",
    "num_layers = 40             \n",
    "layer_width = 1 \n",
    "max_dilation = 15 \n",
    "normalization = nn.BatchNorm2d\n",
    "\n",
    "tunet3 = MultiTUNet(image_shape=(train_imgs.shape[2:4]),\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            depth=depth,\n",
    "            kernel_down=nn.MaxPool2d,\n",
    "            base_channels=base_channels,\n",
    "            normalization = nn.BatchNorm2d,\n",
    "            growth_rate=growth_rate,\n",
    "            hidden_rate=hidden_rate\n",
    "            )\n",
    "print('Number of parameters: ', helpers.count_parameters(tunet3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831490c",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e810f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "print(LEARNING_RATE)\n",
    "optimizer_tunet3 = optim.Adam(tunet3.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1aae10",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = torch.FloatTensor([1,2,2,5]).to(device)\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)   # For segmenting >2 classes\n",
    "criterion = nn.CrossEntropyLoss()   # For segmenting >2 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a17b2",
   "metadata": {},
   "source": [
    "## Device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f161a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = helpers.get_device()\n",
    "device = \"cuda:1\"\n",
    "print('Device we will compute on: ', device)   # cuda:0 for GPU. Else, CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15895243",
   "metadata": {},
   "source": [
    "### load model onto device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunet3.to(device)   # send network to GPU\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985db2f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab0544",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b27b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "experiments = os.path.join(basedir, \"Experiments\")\n",
    "if not os.path.exists(experiments):\n",
    "    os.makedirs(experiments)\n",
    "newds_path = os.path.join(experiments,'Results_TauBin2_MultiUNet_Droput_TrainDown4x_8ZEROSlices_DataFlips')\n",
    "#if os.path.isdir(newds_path) is False: os.mkdir(newds_path)\n",
    "if not os.path.exists(newds_path):\n",
    "    os.makedirs(newds_path)\n",
    "model_msdnet = '/msdnet'\n",
    "model_tunet3 = '/tunet3'\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "main_dir = newds_path + model_tunet3\n",
    "if os.path.isdir(main_dir) is False: os.mkdir(main_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60   # Set number of epochs\n",
    "\n",
    "stepsPerEpoch = np.ceil(train_imgs.shape[0]/batch_size_train)\n",
    "num_steps_down = 2\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_tunet3,\n",
    "                                 step_size=int(stepsPerEpoch*(epochs/num_steps_down)),\n",
    "                                 gamma = 0.1,verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829bc70",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "start = time.time()\n",
    "tunet3, results = train_scripts.train_segmentation(\n",
    "    tunet3,train_loader, val_loader, epochs, \n",
    "    criterion, optimizer_tunet3, device,saveevery=3,\n",
    "    #scheduler=scheduler,\n",
    "    show=1)   # training happens here\n",
    "\n",
    "print(\"Training Time \", time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d62c8",
   "metadata": {},
   "source": [
    "## Training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da82204",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.plot(results['F1 training macro'], linewidth=2, label='training')\n",
    "plt.plot(results['F1 validation macro'], linewidth=2, label='validation')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('TUnet with ReLU and BatchNorm')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(main_dir + '/losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9aade6",
   "metadata": {},
   "source": [
    "## Store trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tunet3.state_dict(), main_dir + '/net')\n",
    "\n",
    "# %%\n",
    "params = {'image_shape': train_imgs.shape[2:4], 'in_channels': in_channels, 'out_channels': out_channels, 'depth': depth, 'base_channels': base_channels, 'growth_rate': growth_rate, 'hidden_rate': hidden_rate},\n",
    "\n",
    "np.save(main_dir+'/params.npy',params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb1d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893847d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ec5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a468616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b40a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e497cd8",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab2384",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(model_type, params):\n",
    "    # set model parameters and initialize the network\n",
    "    if model_type == \"SMSNet\":\n",
    "        net = SMSNet.random_SMS_network(**params)\n",
    "        model_params = {\n",
    "          \"in_channels\": net.in_channels,\n",
    "          \"out_channels\": net.out_channels,\n",
    "          \"in_shape\": net.in_shape,\n",
    "          \"out_shape\": net.out_shape,\n",
    "          \"scaling_table\": net.scaling_table,\n",
    "          \"network_graph\": net.network_graph,\n",
    "          \"channel_count\": net.channel_count,\n",
    "          \"convolution_kernel_size\": net.convolution_kernel_size,\n",
    "          \"first_action\": net.first_action,\n",
    "          \"hidden_action\": net.hidden_action,\n",
    "          \"last_action\":net.last_action,\n",
    "        }\n",
    "        return net, model_params\n",
    "    elif model_type == \"MSDNet\":\n",
    "        net = msdnet.MixedScaleDenseNetwork(**params)\n",
    "        return net, params\n",
    "    elif model_type == 'TUNet':\n",
    "        net = tunet.TUNet(**params)\n",
    "        return net, params\n",
    "    elif model_type == 'MultiTUNet':\n",
    "        net = MultiTUNet(**params)\n",
    "        return net, params\n",
    "    else:\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display(array1, array2):\n",
    "    \"\"\"\n",
    "    Displays ten random images from each one of the supplied arrays.\n",
    "    \"\"\"\n",
    "    n = 7\n",
    "\n",
    "    indices = np.random.randint(len(array1), size=n)\n",
    "    print('The indices of the images are ', indices)\n",
    "    images1 = array1[indices, :]\n",
    "    images2 = array2[indices, :]\n",
    "    plt.figure(figsize=(50, 20))\n",
    "    \n",
    "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(image1, vmin=0, vmax=1)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(image2, vmin=0, vmax=1)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a61b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics( preds, target):\n",
    "    tmp = corcoef.cc(preds.cpu().flatten(), target.cpu().flatten() )\n",
    "    return(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# results_dir = \"/data/Chromatin/MultiScale/Paper/Base/TS_0005/Experiments/Results_TS0005_MultiTUNet_NoDropOut_NoWeights_Test5/tunet3\"\n",
    "\n",
    "results_dir = main_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fcfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.load(results_dir + '/params.npy', allow_pickle=True)\n",
    "params = params[0]\n",
    "print('The following define the network parameters: ', params)\n",
    "\n",
    "\n",
    "# %%\n",
    "# model_type = 'TUNet'\n",
    "#model_type = 'MSDNet'\n",
    "model_type = 'MultiTUNet'  \n",
    "\n",
    "net, model_params = create_network(model_type, params)\n",
    "net.load_state_dict(torch.load(results_dir + '/net'))\n",
    "\n",
    "\n",
    "# %%\n",
    "device = helpers.get_device()\n",
    "device='cuda:1'\n",
    "print('Device we compute on: ', device)\n",
    "print('Number of parameters: ', helpers.count_parameters(net))\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34124494",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"/data/Chromatin/MultiScale/Paper/Tau/TS30_wbp_bin2_flipped/images\"\n",
    "out_folder = '/data/Chromatin/MultiScale/Paper/Base/Tau_bin2/Experiments/outputs_TauBin2_MultiUNet_Droput_TrainDown4x_8ZEROSlices_DataFlips'\n",
    "if os.path.isdir(f'{out_folder}') is False: os.mkdir(f'{out_folder}')\n",
    "\n",
    "\n",
    "# %%\n",
    "files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
    "print('Number of files to segment: ', len(files))\n",
    "files.sort()\n",
    "\n",
    "\n",
    "# %%\n",
    "# test_imgs = np.array(imread(images_file)).astype('float32')\n",
    "test_imgs = []\n",
    "for file in files:\n",
    "#     #img = Image.open(f'{images_dir}/{file}')\n",
    "#     #img.load()\n",
    "    img = cv2.imread(f'{images_dir}/{file}', cv2.IMREAD_GRAYSCALE)\n",
    "    r,c = img.shape\n",
    "    #print(img.shape)\n",
    "    #img = cv2.pyrDown(img, dstsize=(c//2, r//2))\n",
    "    down_img = cv2.pyrDown(img, dstsize=(c//2, r//2))\n",
    "    img = cv2.pyrDown(down_img, dstsize=(c//4, r//4))\n",
    "    img = np.array(img, dtype='float32')\n",
    "#     # Uncomment this if images are RGB \n",
    "#     #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    test_imgs.append(img)\n",
    "test_imgs = np.array(test_imgs)\n",
    "imwrite(\"/data/Chromatin/MultiScale/Paper/Base/Tau/Tau_imgs.tif\", test_imgs)\n",
    "test_imgs = np.expand_dims(np.array(test_imgs), axis=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "print(np.unique(test_imgs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "quilt = qlty2D.NCYXQuilt(X=test_imgs.shape[3],\n",
    "                         Y=test_imgs.shape[2],\n",
    "                         window=(256,256),\n",
    "                         step=(64,64),\n",
    "                         border=(10,10),\n",
    "                         border_weight=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ade3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageSplit(quilt,test_imgs):\n",
    "    dicedImgs = []\n",
    "    labeled_imgs = torch.Tensor(test_imgs)\n",
    "    #print(labeled_imgs.shape)\n",
    "    labeled_imgs = quilt.unstitch(labeled_imgs)\n",
    "    #print(\"x shape: \",test_imgs.shape)\n",
    "    #print(\"x_bits shape:\", labeled_imgs.shape)\n",
    "    \n",
    "    for i in range(len(labeled_imgs)):\n",
    "        bilateral = cv2.bilateralFilter(labeled_imgs[i][0].numpy(),5,50,10)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3)\n",
    "        bilateral= bilateral.astype(np.uint16)\n",
    "        final = clahe.apply(bilateral)\n",
    "        x = exposure.equalize_hist(final)\n",
    "        dicedImgs.append(x.astype(np.float32))\n",
    "        #dicedImgs.append(final.astype(np.float32))\n",
    "    return np.expand_dims(np.array(dicedImgs), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2adeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_imgs(testloader, net):\n",
    "    \"\"\" Modified for input and no ground truth\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    seg_imgs = []\n",
    "    noisy_imgs = []\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            noisy = batch\n",
    "            noisy = noisy[0]\n",
    "            #noisy = normalize(noisy)\n",
    "            noisy = torch.FloatTensor(noisy)\n",
    "            noisy = noisy.to(device)\n",
    "            output = net(noisy)\n",
    "            output = F.softmax(output, dim=1)\n",
    "            if counter == 0:\n",
    "                seg_imgs = output.detach().cpu()\n",
    "                noisy_imgs = noisy.detach().cpu()\n",
    "            else:\n",
    "                seg_imgs = torch.cat((seg_imgs, output.detach().cpu()), 0)\n",
    "                noisy_imgs = torch.cat((noisy_imgs, noisy.detach().cpu()), 0)\n",
    "                \n",
    "            counter+=1\n",
    "            del output\n",
    "            del noisy\n",
    "            torch.cuda.empty_cache()\n",
    "    return seg_imgs, noisy_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20688241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_stack(imgx, imgy, imgz, d_type = None, return_stacks=True):\n",
    "    imgx_stack = []\n",
    "    imgy_stack = []\n",
    "    imgz_stack = []\n",
    "\n",
    "    for j in tqdm(range(len(imgx))):\n",
    "        ix = Image.open(imgx[j])\n",
    "        iy = Image.open(imgy[j])\n",
    "        iz = Image.open(imgz[j])\n",
    "        \n",
    "        ix.load()\n",
    "        iy.load()\n",
    "        iz.load()\n",
    "\n",
    "        if d_type == None:\n",
    "            ix = np.array(ix)\n",
    "            iy = np.array(iy)\n",
    "            iz = np.array(iz)\n",
    "        else:\n",
    "            ix = np.array(ix, dtype=d_type)\n",
    "            iy = np.array(iy, dtype=d_type)\n",
    "            iz = np.array(iz, dtype=d_type)\n",
    "\n",
    "        imgx_stack.append(ix)\n",
    "        imgy_stack.append(iy)\n",
    "        imgz_stack.append(iz)\n",
    "\n",
    "    imgx_stack = np.array(imgx_stack)\n",
    "    imgy_stack = np.array(imgy_stack)\n",
    "    imgz_stack = np.array(imgz_stack)\n",
    "        \n",
    "    if return_stacks == True:\n",
    "        return imgx_stack, imgy_stack, imgz_stack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_mapper = {1:\"Tau\", 2:\"Membrane\", 3:\"Ribosomes\"}\n",
    "file_batch=5\n",
    "out_masks = None \n",
    "\n",
    "\n",
    "# %%\n",
    "for k,v in output_mapper.items():\n",
    "    if os.path.isdir(f'{out_folder}/{v}') is False: os.mkdir(f'{out_folder}/{v}')\n",
    "    if os.path.isdir(f'{out_folder}/{v}/segments') is False: os.mkdir(f'{out_folder}/{v}/segments')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(0,test_imgs.shape[0],file_batch)):\n",
    "    imgs = test_imgs[i:i+file_batch]\n",
    "    #print(imgs.shape)\n",
    "    dicedtestImgs = imageSplit(quilt, imgs)\n",
    "    \n",
    "    batch_size = file_batch\n",
    "    num_workers = 0    #increase to 1 or 2 with multiple GPUs\n",
    "    test_data = TensorDataset(torch.Tensor(dicedtestImgs))\n",
    "    test_loader_params = {'batch_size': batch_size,\n",
    "                     'shuffle': False,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "    test_loader = DataLoader(test_data, **test_loader_params)  \n",
    "    \n",
    "    output, input_imgs  = segment_imgs(test_loader, net)\n",
    "    stitched_output = quilt.stitch(torch.tensor(output))\n",
    "    o = torch.squeeze(stitched_output[0], 1)\n",
    "    tunet3_output = torch.argmax(o.cpu()[:,:,:,:].data, dim=1)\n",
    "    \n",
    "    masks=tunet3_output.numpy()\n",
    "    imgs= np.squeeze(imgs,1)\n",
    "    \n",
    "    out_masks=masks if out_masks is None else np.vstack((out_masks,masks))\n",
    "    \n",
    "    for k,v in output_mapper.items():\n",
    "        idx=(masks==k)\n",
    "        structures=np.zeros(imgs.shape)\n",
    "        structures[idx]=imgs[idx]\n",
    "        out_path = f'{out_folder}/{v}/segments/'\n",
    "        \n",
    "        for j in range(structures.shape[0]):\n",
    "            name = f'{i+j:04}.jpg'\n",
    "            #print(out_path+name)\n",
    "            Image.fromarray(structures[j].astype(np.uint8)).save(out_path+name)\n",
    "        \n",
    "    del output\n",
    "    del tunet3_output\n",
    "    del input_imgs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "imwrite(out_folder+'/masks.tif', np.array(out_masks, 'uint8'))\n",
    "print(\"Testing Time \", time.time()-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ce725",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_size = 100 # remove objects smaller than this size. \n",
    "\n",
    "def clean_stack(img_stack, minim):\n",
    "        cleaned = np.copy(img_stack)\n",
    "        cleaned_index = (cleaned!=0)\n",
    "        for j in tqdm(range(len(cleaned))):\n",
    "            img = cleaned_index[j,:] \n",
    "            img = morphology.remove_small_objects(img, minim, connectivity=1)\n",
    "            target_img = cleaned[j,:,:]\n",
    "            cleaned[j,:,:] = np.multiply(target_img, img)\n",
    "        return cleaned\n",
    "\n",
    "for k,v in output_mapper.items():\n",
    "    path = f'{folder}/{v}/segments'\n",
    "        \n",
    "    files = []\n",
    "    for file in glob.glob(path+\"/*.jpg\"):files.append(file)\n",
    "    files = sorted(files)\n",
    "    imgs= []\n",
    "    for j in range(len(files)):\n",
    "        img = Image.open(files[j])\n",
    "        img.load()\n",
    "        img = np.array(img, dtype='float32')\n",
    "        imgs.append(img)\n",
    "    imwrite(f'{folder}/{v}/{v}.tiff', clean_stack(imgs, object_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4f117",
   "metadata": {},
   "source": [
    "## Generate Co-ordinates for subtomo averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9caf520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_points(arr,z):\n",
    "    \n",
    "    final_coord = []\n",
    "    \n",
    "    count = collections.defaultdict(list)\n",
    "    rows,cols = np.nonzero(arr)\n",
    "    \n",
    "    for r,c in zip(list(rows),list(cols)):\n",
    "        pixel = arr[r][c]\n",
    "        count[pixel].append([r,c])\n",
    "        \n",
    "    for pixel, coord in count.items():\n",
    "        \n",
    "        simplied = rdp.rdp_iter(np.array(coord),epsilon=0.5)\n",
    "        simplied = [(x[0],x[1],z,pixel) for x in simplied]\n",
    "        final_coord += simplied \n",
    "    \n",
    "    return final_coord\n",
    "\n",
    "for k,v in output_mapper.items():\n",
    "    file =f'{folder}/{v}/{v}.tiff'\n",
    "    \n",
    "    imgs =imread(file)\n",
    "    imgs[imgs!=0]=1\n",
    "    labels_out= cc3d.connected_components(imgs, connectivity=6)\n",
    "    total,rows,cols = labels_out.shape\n",
    "\n",
    "    with open(f'{folder}/{v}/coordinates.csv','a',encoding='UTF8',newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for images in range(total):\n",
    "            for val in simplify_points(labels_out[images],images):\n",
    "                writer.writerow(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02db774",
   "metadata": {},
   "source": [
    "\n",
    "## Simplify co-ordinates\n",
    "\n",
    "### Generate only one co-ordinate per feature(generates one co-ordinate for each filament or ribosome)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv(f'{folder}/{v}/coordinates.csv',names=[\"x\",\"y\",\"z\",\"pixel\"])\n",
    "grouped = df.groupby('pixel')\n",
    "random_points = grouped.apply(lambda x: x.iloc[np.random.randint(0,len(x))])\n",
    "random_points.to_csv(f'{folder}/{v}/simplified_coord.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88316411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d6e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b4ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8427013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73c3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df6f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c646792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7eb0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3bd30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c33f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15592f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401c4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6d205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
